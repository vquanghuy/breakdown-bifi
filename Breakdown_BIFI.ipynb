{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMh9r11F0BD9"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxK14PmC9Yzz"
   },
   "source": [
    "We should check if we're on the Colab and do additional setup\n",
    "- Install `fairseq`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ngsjzyFax4KL",
    "outputId": "f268dfe5-a44b-4063-f616-818e600b9d75",
    "ExecuteTime": {
     "end_time": "2024-07-11T04:23:21.301092Z",
     "start_time": "2024-07-11T04:23:21.287094Z"
    }
   },
   "source": [
    "from IPython.core import getipython\n",
    "\n",
    "is_colab = 'google.colab' in str(getipython.get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "  !pip install fairseq tqdm\n",
    "  print(\"Fairseq installation successful (if no errors occurred)\")\n",
    "else:\n",
    "  print(\"Notebook is not on Colab. Fairseq installation not attempted.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is not on Colab. Fairseq installation not attempted.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rAuWy859T45",
    "outputId": "ec2cd9b7-7e6b-4c05-8775-4c47b84bb5fb",
    "ExecuteTime": {
     "end_time": "2024-07-11T04:23:21.285091Z",
     "start_time": "2024-07-11T04:23:19.003181Z"
    }
   },
   "source": [
    "# Check PyTorch version\n",
    "import torch\n",
    "print(torch.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNBdpnZp_bQt"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0q3s-dVP_L00",
    "ExecuteTime": {
     "end_time": "2024-07-11T04:24:59.137334Z",
     "start_time": "2024-07-11T04:24:59.121320Z"
    }
   },
   "source": "DATA_DIR = 'drive/MyDrive/Dataset/bifi-dataset' if is_colab else 'data'",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38ITJmmMBrcQ"
   },
   "source": [
    "# Supported functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "acqFRA0EHxRg",
    "ExecuteTime": {
     "end_time": "2024-07-10T13:34:23.531874Z",
     "start_time": "2024-07-10T13:34:23.518409Z"
    }
   },
   "source": [
    "import shlex\n",
    "import subprocess\n",
    "import sys"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qUgoaHVGnQy"
   },
   "source": [
    "## fairseq_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KHeGjQtqBtul",
    "ExecuteTime": {
     "end_time": "2024-07-11T04:24:54.488850Z",
     "start_time": "2024-07-11T04:24:54.472851Z"
    }
   },
   "source": [
    "def fairseq_preprocess(src, tgt, destdir, trainpref=None, validpref=None, testpref=None, srcdict=None, **kwargs):\n",
    "    additional_cmds = ''.join([f\"--{k.replace('_', '-')} {v} \" for k, v in kwargs.items() if not isinstance(v, bool)])\n",
    "    additional_cmds += ''.join([f\"--{k.replace('_', '-')} \" for k, v in kwargs.items() if isinstance(v, bool) and v])\n",
    "    cmd = f'fairseq-preprocess --source-lang {src} --destdir {destdir} \\\n",
    "            --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 '\n",
    "    if tgt is not None:\n",
    "        cmd += f'--target-lang {tgt} '\n",
    "    if trainpref is not None:\n",
    "        cmd += f'--trainpref {trainpref} '\n",
    "    if validpref is not None:\n",
    "        cmd += f'--validpref {validpref} '\n",
    "    if testpref is not None:\n",
    "        cmd += f'--testpref {testpref} '\n",
    "    if srcdict is not None:\n",
    "        cmd += f'--srcdict {srcdict} '\n",
    "    cmd += additional_cmds\n",
    "\n",
    "    # Execute command line command\n",
    "    print(cmd)\n",
    "    !{cmd}"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALPMFOrMGok3"
   },
   "source": [
    "## fairseq_train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AGDCnx3zBukX",
    "ExecuteTime": {
     "end_time": "2024-07-10T14:46:29.683082Z",
     "start_time": "2024-07-10T14:46:29.647530Z"
    }
   },
   "source": [
    "def fairseq_train(GPUs, preprocess_dir, save_dir, logfile, src, tgt, model='transformer',\n",
    "                  criterion='label_smoothed_cross_entropy',\n",
    "                  encoder_layers=4, decoder_layers=4, encoder_embed_dim=256,\n",
    "                  decoder_embed_dim=256, encoder_ffn_embed_dim=1024,\n",
    "                  decoder_ffn_embed_dim=1024, encoder_attention_heads=8,\n",
    "                  decoder_attention_heads=8, dropout=0.4,\n",
    "                  attention_dropout=0.2, relu_dropout=0.2,\n",
    "                  weight_decay=0.0001, warmup_updates=400, warmup_init_lr=1e-4,\n",
    "                  lr=1e-3, min_lr=1e-9, max_tokens=1000, update_freq=4,\n",
    "                  max_epoch=10, save_interval=1, log_interval=100, log_format='tqdm',\n",
    "                  user_dir=None, reset=False, restore_file=None, **kwargs):\n",
    "    if True:\n",
    "        additional_cmds = ''.join(\n",
    "            [f\"--{k.replace('_', '-')} {v} \" for k, v in kwargs.items() if not isinstance(v, bool)])\n",
    "        additional_cmds += ''.join(\n",
    "            [f\"--{k.replace('_', '-')} \" for k, v in kwargs.items() if isinstance(v, bool) and v])\n",
    "        cmd = f\"fairseq-train \\\n",
    "                {preprocess_dir} \\\n",
    "               --source-lang {src} --target-lang {tgt} \\\n",
    "               --arch {model} --share-all-embeddings \\\n",
    "               --encoder-layers {encoder_layers} --decoder-layers {decoder_layers} \\\n",
    "               --encoder-embed-dim {encoder_embed_dim} --decoder-embed-dim {decoder_embed_dim} \\\n",
    "               --encoder-ffn-embed-dim {encoder_ffn_embed_dim} --decoder-ffn-embed-dim {decoder_ffn_embed_dim} \\\n",
    "               --encoder-attention-heads {encoder_attention_heads} --decoder-attention-heads {decoder_attention_heads} \\\n",
    "               --encoder-normalize-before --decoder-normalize-before \\\n",
    "               --dropout {dropout} --attention-dropout {attention_dropout} --relu-dropout {relu_dropout} \\\n",
    "               --weight-decay {weight_decay} \\\n",
    "               --criterion {criterion} \\\n",
    "               --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1 \\\n",
    "               --lr-scheduler inverse_sqrt --warmup-updates {warmup_updates} --warmup-init-lr {warmup_init_lr} \\\n",
    "               --lr {lr} --min-lr {min_lr} \\\n",
    "               --max-tokens {max_tokens} \\\n",
    "               --update-freq {update_freq} \\\n",
    "               --max-epoch {max_epoch} --save-interval {save_interval} --save-dir {save_dir} \"\n",
    "        if user_dir is not None:\n",
    "            cmd += f'--user-dir {user_dir} '\n",
    "        if restore_file is not None:\n",
    "            cmd += f\"--restore-file {restore_file} \"\n",
    "        if reset:\n",
    "            cmd += \"--reset-optimizer \\\n",
    "                   --reset-lr-scheduler \\\n",
    "                   --reset-dataloader \\\n",
    "                   --reset-meters \"\n",
    "        cmd += additional_cmds\n",
    "        if logfile is not None:\n",
    "            import socket, os\n",
    "            with open(logfile, 'w') as outf:\n",
    "                print(socket.gethostname(), file=outf)\n",
    "                print(\"pid:\", os.getpid(), file=outf)\n",
    "                print(\"screen: %s\" % subprocess.check_output('echo $STY', shell=True).decode('utf'), file=outf)\n",
    "                outf.flush()\n",
    "            cmd += f\"  2>&1 | tee -a {logfile} \"\n",
    "        if GPUs is not None:\n",
    "            cmd = 'CUDA_VISIBLE_DEVICES={}  {}'.format(GPUs, cmd)\n",
    "\n",
    "        print(cmd)\n",
    "        #!{cmd}cmd"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPrCdDrwB14d"
   },
   "source": [
    "# Round 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SWzdVzcktvpx",
    "ExecuteTime": {
     "end_time": "2024-07-11T04:24:43.817309Z",
     "start_time": "2024-07-11T04:24:43.134570Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(DATA_DIR)\n",
    "round_dir = data_dir/'round0'\n",
    "data_paired_dir = round_dir/'data_paired'\n",
    "fairseq_dir = data_paired_dir/'fairseq_preprocess'"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m----> 3\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m Path(\u001B[43mDATA_DIR\u001B[49m)\n\u001B[0;32m      4\u001B[0m round_dir \u001B[38;5;241m=\u001B[39m data_dir\u001B[38;5;241m/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mround0\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      5\u001B[0m data_paired_dir \u001B[38;5;241m=\u001B[39m round_dir\u001B[38;5;241m/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_paired\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'DATA_DIR' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkKlR8UWGSJR"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autwpxOFM8kE"
   },
   "outputs": [],
   "source": [
    "# Remove fairseq dir\n",
    "!rm -r {str(fairseq_dir)}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZs7SU74DRw6",
    "outputId": "bf8a615f-209e-460a-930f-b340c97bcb5b",
    "ExecuteTime": {
     "end_time": "2024-07-10T13:47:24.210145Z",
     "start_time": "2024-07-10T13:39:35.638770Z"
    }
   },
   "source": [
    "fairseq_preprocess(src='bad', tgt='good', workers=20,\n",
    "                      destdir  = str(data_paired_dir/'fairseq_preprocess'),\n",
    "                      trainpref= str(data_paired_dir/'train'),\n",
    "                      validpref= str(data_paired_dir/'dev'),\n",
    "                      srcdict  = str(data_dir/'token_vocab.txt') )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-preprocess --source-lang bad --destdir data\\round0\\data_paired\\fairseq_preprocess             --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 --target-lang good --trainpref data\\round0\\data_paired\\train --validpref data\\round0\\data_paired\\dev --srcdict data\\token_vocab.txt --workers 20 \n",
      "^C\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9pY-SAZGVpP"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nlaNGVmD_V6",
    "outputId": "5c41bf42-7fe9-48ab-e0b8-9a403c96c828",
    "ExecuteTime": {
     "end_time": "2024-07-10T14:46:48.206121Z",
     "start_time": "2024-07-10T14:46:48.147013Z"
    }
   },
   "source": [
    "# Train\n",
    "# --gpu_id 0 --max_epoch 2\n",
    "gpu_id = 0\n",
    "max_epoch = 2\n",
    "\n",
    "save_dir = round_dir/'model-fixer'\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "fairseq_train(gpu_id, str(fairseq_dir), str(save_dir), str(save_dir/'train.log.txt'),\n",
    "                    src='bad', tgt='good',\n",
    "                    criterion='label_smoothed_cross_entropy', label_smoothing=0.1,\n",
    "                    lr=1e-3, warmup_init_lr=1e-4, memory_efficient_fp16=True,\n",
    "                    encoder_layers=4, decoder_layers=4, encoder_embed_dim=256, decoder_embed_dim=256,\n",
    "                    encoder_ffn_embed_dim=1024, decoder_ffn_embed_dim=1024,\n",
    "                    max_tokens=13500, update_freq=2,\n",
    "                    max_epoch=max_epoch, save_interval_updates=10000, num_workers=4,\n",
    "                )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0  fairseq-train                 data\\round0\\data_paired\\fairseq_preprocess                --source-lang bad --target-lang good                --arch transformer --share-all-embeddings                --encoder-layers 4 --decoder-layers 4                --encoder-embed-dim 256 --decoder-embed-dim 256                --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024                --encoder-attention-heads 8 --decoder-attention-heads 8                --encoder-normalize-before --decoder-normalize-before                --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2                --weight-decay 0.0001                --criterion label_smoothed_cross_entropy                --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1                --lr-scheduler inverse_sqrt --warmup-updates 400 --warmup-init-lr 0.0001                --lr 0.001 --min-lr 1e-09                --max-tokens 13500                --update-freq 2                --max-epoch 2 --save-interval 1 --save-dir data\\round0\\model-fixer --label-smoothing 0.1 --save-interval-updates 10000 --num-workers 4 --memory-efficient-fp16   2>&1 | tee -a data\\round0\\model-fixer\\train.log.txt \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwxK8uD3HQKe"
   },
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N0MH3xgJHPhG",
    "outputId": "d5261d93-3c34-4626-830b-af445d514fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fairseq-preprocess [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                          [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE]\n",
      "                          [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]\n",
      "                          [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                          [--wandb-project WANDB_PROJECT] [--azureml-logging] [--seed SEED]\n",
      "                          [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16] [--fp16]\n",
      "                          [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
      "                          [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                          [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                          [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                          [--on-cpu-convert-precision] [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                          [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                          [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                          [--amp-init-scale AMP_INIT_SCALE] [--amp-scale-window AMP_SCALE_WINDOW]\n",
      "                          [--user-dir USER_DIR] [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                          [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                          [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                          [--quantization-config-path QUANTIZATION_CONFIG_PATH] [--profile]\n",
      "                          [--reset-logging] [--suppress-crashes] [--use-plasma-view]\n",
      "                          [--plasma-path PLASMA_PATH]\n",
      "                          [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                          [--tokenizer {moses,nltk,space}]\n",
      "                          [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                          [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                          [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                          [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}] [--task TASK]\n",
      "                          [-s SRC] [-t TARGET] [--trainpref FP] [--validpref FP] [--testpref FP]\n",
      "                          [--align-suffix FP] [--destdir DIR] [--thresholdtgt N]\n",
      "                          [--thresholdsrc N] [--tgtdict FP] [--srcdict FP] [--nwordstgt N]\n",
      "                          [--nwordssrc N] [--alignfile ALIGN] [--dataset-impl FORMAT]\n",
      "                          [--joined-dictionary] [--only-source] [--padding-factor N] [--workers N]\n",
      "                          [--dict-only]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --no-progress-bar     disable progress bar\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        log progress every N batches (when progress bar is disabled)\n",
      "  --log-format {json,none,simple,tqdm}\n",
      "                        log format to use\n",
      "  --log-file LOG_FILE   log file to copy metrics to.\n",
      "  --aim-repo AIM_REPO   path to Aim repository\n",
      "  --aim-run-hash AIM_RUN_HASH\n",
      "                        Aim run hash. If skipped, creates or continues run based on save_dir\n",
      "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
      "                        path to save logs for tensorboard, should match --logdir of running\n",
      "                        tensorboard (default: no tensorboard logging)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Weights and Biases project name to use for logging\n",
      "  --azureml-logging     Log scalars to AzureML context\n",
      "  --seed SEED           pseudo random number generator seed\n",
      "  --cpu                 use CPU instead of CUDA\n",
      "  --tpu                 use TPU instead of CUDA\n",
      "  --bf16                use bfloat16; implies --tpu\n",
      "  --memory-efficient-bf16\n",
      "                        use a memory-efficient version of BF16 training; implies --bf16\n",
      "  --fp16                use FP16\n",
      "  --memory-efficient-fp16\n",
      "                        use a memory-efficient version of FP16 training; implies --fp16\n",
      "  --fp16-no-flatten-grads\n",
      "                        don't flatten FP16 grads tensor\n",
      "  --fp16-init-scale FP16_INIT_SCALE\n",
      "                        default FP16 loss scale\n",
      "  --fp16-scale-window FP16_SCALE_WINDOW\n",
      "                        number of updates before increasing loss scale\n",
      "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
      "                        pct of updates that can overflow before decreasing the loss scale\n",
      "  --on-cpu-convert-precision\n",
      "                        if set, the floating point conversion to fp16/bf16 runs on CPU. This\n",
      "                        reduces bus transfer time and GPU memory usage.\n",
      "  --min-loss-scale MIN_LOSS_SCALE\n",
      "                        minimum FP16/AMP loss scale, after which training is stopped\n",
      "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
      "                        threshold FP16 loss scale from below\n",
      "  --amp                 use automatic mixed precision\n",
      "  --amp-batch-retries AMP_BATCH_RETRIES\n",
      "                        number of retries of same batch after reducing loss scale with AMP\n",
      "  --amp-init-scale AMP_INIT_SCALE\n",
      "                        default AMP loss scale\n",
      "  --amp-scale-window AMP_SCALE_WINDOW\n",
      "                        number of updates before increasing AMP loss scale\n",
      "  --user-dir USER_DIR   path to a python module containing custom extensions (tasks and/or\n",
      "                        architectures)\n",
      "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
      "                        how often to clear the PyTorch CUDA cache (0 to disable)\n",
      "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
      "                        number of bytes reserved for gathering stats from workers\n",
      "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
      "                        total number of GPUs to parallelize model over\n",
      "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
      "                        path to quantization config file\n",
      "  --profile             enable autograd profiler emit_nvtx\n",
      "  --reset-logging       when using Hydra, reset the logging at the beginning of training\n",
      "  --suppress-crashes    suppress crashes when training with the hydra_train entry point so that\n",
      "                        the main method can return a value (useful for sweeps)\n",
      "  --use-plasma-view     Store indices and sizes in shared memory\n",
      "  --plasma-path PLASMA_PATH\n",
      "                        path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend\n",
      "                        to fail.\n",
      "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
      "  --tokenizer {moses,nltk,space}\n",
      "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
      "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
      "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
      "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
      "  --task TASK           task\n",
      "  --dataset-impl FORMAT\n",
      "                        output dataset implementation\n",
      "\n",
      "Preprocessing:\n",
      "  -s SRC, --source-lang SRC\n",
      "                        source language\n",
      "  -t TARGET, --target-lang TARGET\n",
      "                        target language\n",
      "  --trainpref FP        train file prefix (also used to build dictionaries)\n",
      "  --validpref FP        comma separated, valid file prefixes (words missing from train set are\n",
      "                        replaced with <unk>)\n",
      "  --testpref FP         comma separated, test file prefixes (words missing from train set are\n",
      "                        replaced with <unk>)\n",
      "  --align-suffix FP     alignment file suffix\n",
      "  --destdir DIR         destination dir\n",
      "  --thresholdtgt N      map words appearing less than threshold times to unknown\n",
      "  --thresholdsrc N      map words appearing less than threshold times to unknown\n",
      "  --tgtdict FP          reuse given target dictionary\n",
      "  --srcdict FP          reuse given source dictionary\n",
      "  --nwordstgt N         number of target words to retain\n",
      "  --nwordssrc N         number of source words to retain\n",
      "  --alignfile ALIGN     an alignment file (optional)\n",
      "  --joined-dictionary   Generate joined dictionary\n",
      "  --only-source         Only process the source language\n",
      "  --padding-factor N    Pad dictionary size to be multiple of N\n",
      "  --workers N           number of parallel workers\n",
      "  --dict-only           if true, only builds a dictionary and then exits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "process = subprocess.Popen([\"fairseq-preprocess\", \"--help\"], stdout=subprocess.PIPE, universal_newlines=True)\n",
    "for line in process.stdout:\n",
    "  print(line, end='')  # Print without newline to avoid extra line breaks\n",
    "  sys.stdout.flush()  # Flush output buffer to display immediately\n",
    "\n",
    "# Wait for the process to finish (optional)\n",
    "process.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C__u2mvGHW83",
    "outputId": "1712e31e-c464-4c59-df1b-8f5db439abd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 06:24:28.857910: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 06:24:28.857974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 06:24:28.859707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 06:24:28.868270: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 06:24:30.350187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-09 06:24:33 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-07-09 06:24:34 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=True, log_interval=20, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='bad', target_lang='good', trainpref='drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/train', validpref='drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/dev', testpref=None, align_suffix=None, destdir='drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='drive/MyDrive/Dataset/bifi-dataset/token_vocab.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "    main(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 306, in main\n",
      "    raise FileExistsError(_dict_path(args.target_lang, args.destdir))\n",
      "FileExistsError: drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess/dict.good.txt\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang bad --destdir drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess             --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 --target-lang good --trainpref drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/train --validpref drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/dev --srcdict drive/MyDrive/Dataset/bifi-dataset/token_vocab.txt --workers 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmum8GzmIqUB",
    "outputId": "9806a5c4-90ed-4589-bf14-eeb8c783f619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 06:24:54.755732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 06:24:54.755815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 06:24:54.757430: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 06:24:54.765945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 06:24:56.417512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-09 06:24:59 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-07-09 06:24:59 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=True, log_interval=20, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='bad', target_lang='good', trainpref='drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/train', validpref='drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/dev', testpref=None, align_suffix=None, destdir='drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='drive/MyDrive/Dataset/bifi-dataset/token_vocab.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-preprocess\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "    main(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/preprocess.py\", line 306, in main\n",
      "    raise FileExistsError(_dict_path(args.target_lang, args.destdir))\n",
      "FileExistsError: drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess/dict.good.txt\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang bad --destdir drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess             --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 --target-lang good --trainpref drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/train --validpref drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/dev --srcdict drive/MyDrive/Dataset/bifi-dataset/token_vocab.txt --workers 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4ZN2xhDKD77",
    "outputId": "7f9ee4e4-f6a4-4137-d213-5ee74b160cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 09:00:37.263366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 09:00:37.263428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 09:00:37.265663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 09:00:37.274412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 09:00:38.704393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-09 09:00:40 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
      "2024-07-09 09:00:42 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                     [--log-format {json,none,simple,tqdm}] [--log-file LOG_FILE]\n",
      "                     [--aim-repo AIM_REPO] [--aim-run-hash AIM_RUN_HASH]\n",
      "                     [--tensorboard-logdir TENSORBOARD_LOGDIR] [--wandb-project WANDB_PROJECT]\n",
      "                     [--azureml-logging] [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                     [--memory-efficient-bf16] [--fp16] [--memory-efficient-fp16]\n",
      "                     [--fp16-no-flatten-grads] [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                     [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE] [--on-cpu-convert-precision]\n",
      "                     [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                     [--amp-batch-retries AMP_BATCH_RETRIES] [--amp-init-scale AMP_INIT_SCALE]\n",
      "                     [--amp-scale-window AMP_SCALE_WINDOW] [--user-dir USER_DIR]\n",
      "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                     [--quantization-config-path QUANTIZATION_CONFIG_PATH] [--profile]\n",
      "                     [--reset-logging] [--suppress-crashes] [--use-plasma-view]\n",
      "                     [--plasma-path PLASMA_PATH]\n",
      "                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                     [--tokenizer {moses,nltk,space}]\n",
      "                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}] [--task TASK]\n",
      "                     [--num-workers NUM_WORKERS] [--skip-invalid-size-inputs-valid-test]\n",
      "                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                     [--data-buffer-size DATA_BUFFER_SIZE] [--train-subset TRAIN_SUBSET]\n",
      "                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]\n",
      "                     [--ignore-unused-valid-subsets] [--validate-interval VALIDATE_INTERVAL]\n",
      "                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                     [--fixed-validation-seed FIXED_VALIDATION_SEED] [--disable-validation]\n",
      "                     [--max-tokens-valid MAX_TOKENS_VALID] [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                     [--max-valid-steps MAX_VALID_STEPS] [--curriculum CURRICULUM]\n",
      "                     [--gen-subset GEN_SUBSET] [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n",
      "                     [--grouped-shuffling] [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                     [--update-ordered-indices-seed]\n",
      "                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                     [--distributed-rank DISTRIBUTED_RANK]\n",
      "                     [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                     [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]\n",
      "                     [--distributed-no-spawn]\n",
      "                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                     [--ddp-comm-hook {none,fp16}] [--bucket-cap-mb BUCKET_CAP_MB]\n",
      "                     [--fix-batches-to-gpus] [--find-unused-parameters]\n",
      "                     [--gradient-as-bucket-view] [--fast-stat-sync]\n",
      "                     [--heartbeat-timeout HEARTBEAT_TIMEOUT] [--broadcast-buffers]\n",
      "                     [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                     [--localsgd-frequency LOCALSGD_FREQUENCY] [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                     [--pipeline-model-parallel] [--pipeline-balance PIPELINE_BALANCE]\n",
      "                     [--pipeline-devices PIPELINE_DEVICES] [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                     [--pipeline-checkpoint {always,never,except_last}]\n",
      "                     [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                     [--fp32-reduce-scatter] [--cpu-offload] [--use-sharded-state]\n",
      "                     [--not-fsdp-flatten-parameters] [--arch ARCH] [--max-epoch MAX_EPOCH]\n",
      "                     [--max-update MAX_UPDATE] [--stop-time-hours STOP_TIME_HOURS]\n",
      "                     [--clip-norm CLIP_NORM] [--sentence-avg] [--update-freq UPDATE_FREQ]\n",
      "                     [--lr LR] [--stop-min-lr STOP_MIN_LR] [--use-bmuf] [--skip-remainder-batch]\n",
      "                     [--save-dir SAVE_DIR] [--restore-file RESTORE_FILE]\n",
      "                     [--continue-once CONTINUE_ONCE] [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                     [--reset-dataloader] [--reset-lr-scheduler] [--reset-meters]\n",
      "                     [--reset-optimizer] [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                     [--save-interval SAVE_INTERVAL]\n",
      "                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                     [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS] [--no-save]\n",
      "                     [--no-epoch-checkpoints] [--no-last-checkpoints] [--no-save-optimizer-state]\n",
      "                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                     [--load-checkpoint-on-all-dp-ranks] [--write-checkpoints-asynchronously]\n",
      "                     [--store-ema] [--ema-decay EMA_DECAY] [--ema-start-update EMA_START_UPDATE]\n",
      "                     [--ema-seed-model EMA_SEED_MODEL] [--ema-update-freq EMA_UPDATE_FREQ]\n",
      "                     [--ema-fp32]\n",
      "                     [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]\n",
      "                     [--dropout DROPOUT] [--attention-dropout ATTENTION_DROPOUT]\n",
      "                     [--activation-dropout ACTIVATION_DROPOUT] [--adaptive-input]\n",
      "                     [--encoder-embed-path ENCODER_EMBED_PATH]\n",
      "                     [--encoder-embed-dim ENCODER_EMBED_DIM]\n",
      "                     [--encoder-ffn-embed-dim ENCODER_FFN_EMBED_DIM]\n",
      "                     [--encoder-layers ENCODER_LAYERS]\n",
      "                     [--encoder-attention-heads ENCODER_ATTENTION_HEADS]\n",
      "                     [--encoder-normalize-before] [--encoder-learned-pos]\n",
      "                     [--encoder-layerdrop ENCODER_LAYERDROP]\n",
      "                     [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]\n",
      "                     [--encoder-xformers-att-config ENCODER_XFORMERS_ATT_CONFIG]\n",
      "                     [--max-source-positions MAX_SOURCE_POSITIONS]\n",
      "                     [--decoder-embed-path DECODER_EMBED_PATH]\n",
      "                     [--decoder-embed-dim DECODER_EMBED_DIM]\n",
      "                     [--decoder-ffn-embed-dim DECODER_FFN_EMBED_DIM]\n",
      "                     [--decoder-layers DECODER_LAYERS]\n",
      "                     [--decoder-attention-heads DECODER_ATTENTION_HEADS]\n",
      "                     [--decoder-normalize-before] [--decoder-learned-pos]\n",
      "                     [--decoder-layerdrop DECODER_LAYERDROP]\n",
      "                     [--decoder-layers-to-keep DECODER_LAYERS_TO_KEEP]\n",
      "                     [--decoder-xformers-att-config DECODER_XFORMERS_ATT_CONFIG]\n",
      "                     [--decoder-output-dim DECODER_OUTPUT_DIM]\n",
      "                     [--max-target-positions MAX_TARGET_POSITIONS]\n",
      "                     [--share-decoder-input-output-embed] [--share-all-embeddings]\n",
      "                     [--no-token-positional-embeddings]\n",
      "                     [--adaptive-softmax-cutoff ADAPTIVE_SOFTMAX_CUTOFF]\n",
      "                     [--adaptive-softmax-dropout ADAPTIVE_SOFTMAX_DROPOUT]\n",
      "                     [--adaptive-softmax-factor ADAPTIVE_SOFTMAX_FACTOR] [--layernorm-embedding]\n",
      "                     [--tie-adaptive-weights] [--tie-adaptive-proj] [--no-scale-embedding]\n",
      "                     [--checkpoint-activations] [--offload-activations] [--no-cross-attention]\n",
      "                     [--cross-self-attention] [--quant-noise-pq QUANT_NOISE_PQ]\n",
      "                     [--quant-noise-pq-block-size QUANT_NOISE_PQ_BLOCK_SIZE]\n",
      "                     [--quant-noise-scalar QUANT_NOISE_SCALAR]\n",
      "                     [--min-params-to-wrap MIN_PARAMS_TO_WRAP] [--char-inputs]\n",
      "                     [--relu-dropout RELU_DROPOUT] [--base-layers BASE_LAYERS]\n",
      "                     [--base-sublayers BASE_SUBLAYERS] [--base-shuffle BASE_SHUFFLE] [--export]\n",
      "                     [--no-decoder-final-norm] [--source-lang SOURCE_LANG]\n",
      "                     [--target-lang TARGET_LANG] [--load-alignments] [--left-pad-source]\n",
      "                     [--left-pad-target] [--upsample-primary UPSAMPLE_PRIMARY] [--truncate-source]\n",
      "                     [--num-batch-buckets NUM_BATCH_BUCKETS] [--eval-bleu]\n",
      "                     [--eval-bleu-args EVAL_BLEU_ARGS] [--eval-bleu-detok EVAL_BLEU_DETOK]\n",
      "                     [--eval-bleu-detok-args EVAL_BLEU_DETOK_ARGS] [--eval-tokenized-bleu]\n",
      "                     [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]] [--eval-bleu-print-samples]\n",
      "                     [--label-smoothing LABEL_SMOOTHING] [--report-accuracy]\n",
      "                     [--ignore-prefix-size IGNORE_PREFIX_SIZE] [--adam-betas ADAM_BETAS]\n",
      "                     [--adam-eps ADAM_EPS] [--weight-decay WEIGHT_DECAY] [--use-old-adam]\n",
      "                     [--fp16-adam-stats] [--warmup-updates WARMUP_UPDATES]\n",
      "                     [--warmup-init-lr WARMUP_INIT_LR] [--pad PAD] [--eos EOS] [--unk UNK]\n",
      "                     data\n",
      "fairseq-train: error: unrecognized arguments: --min-lr 1e-09\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0  fairseq-train                 drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess                --source-lang bad --target-lang good                --arch transformer --share-all-embeddings                --encoder-layers 4 --decoder-layers 4                --encoder-embed-dim 256 --decoder-embed-dim 256                --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024                --encoder-attention-heads 8 --decoder-attention-heads 8                --encoder-normalize-before --decoder-normalize-before                --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2                --weight-decay 0.0001                --criterion label_smoothed_cross_entropy                --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1                --lr-scheduler inverse_sqrt --warmup-updates 400 --warmup-init-lr 0.0001                --lr 0.001 --min-lr 1e-09                --max-tokens 13500                --update-freq 2                --max-epoch 2 --save-interval 1 --save-dir drive/MyDrive/Dataset/bifi-dataset/round_0/model-fixer --label-smoothing 0.1 --save-interval-updates 10000 --num-workers 4 --memory-efficient-fp16   2>&1 | tee -a drive/MyDrive/Dataset/bifi-dataset/round_0/model-fixer/train.log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DwuGvgWuSRt",
    "outputId": "0226d9fa-075b-473d-ca2b-86658fad7ae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-09 11:10:48.377172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-09 11:10:48.377247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-09 11:10:48.379746: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-09 11:10:48.389850: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 11:10:49.991014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-07-09 11:10:51 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
      "2024-07-09 11:10:53 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-07-09 11:10:57 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 0, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 13500, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 13500, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'drive/MyDrive/Dataset/bifi-dataset/round_0/model-fixer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 10000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=True, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=4, skip_invalid_size_inputs_valid_test=False, max_tokens=13500, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=13500, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=2, max_update=0, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[2], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='drive/MyDrive/Dataset/bifi-dataset/round_0/model-fixer', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=10000, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='./drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess', source_lang='bad', target_lang='good', load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=400, warmup_init_lr=0.0001, pad=1, eos=2, unk=3, share_all_embeddings=True, encoder_layers=4, decoder_layers=4, encoder_embed_dim=256, decoder_embed_dim=256, encoder_ffn_embed_dim=1024, decoder_ffn_embed_dim=1024, encoder_attention_heads=8, decoder_attention_heads=8, encoder_normalize_before=True, decoder_normalize_before=True, dropout=0.4, attention_dropout=0.2, relu_dropout=0.2, no_seed_provided=False, encoder_embed_path=None, encoder_learned_pos=False, decoder_embed_path=None, decoder_learned_pos=False, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=256, decoder_input_dim=256, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': './drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess', 'source_lang': 'bad', 'target_lang': 'good', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 400, 'warmup_init_lr': 0.0001, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2024-07-09 11:10:57 | INFO | fairseq.tasks.translation | [bad] dictionary: 10000 types\n",
      "2024-07-09 11:10:57 | INFO | fairseq.tasks.translation | [good] dictionary: 10000 types\n",
      "2024-07-09 11:10:58 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(10000, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(10000, 256, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_projection): Linear(in_features=256, out_features=10000, bias=False)\n",
      "  )\n",
      ")\n",
      "2024-07-09 11:10:58 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2024-07-09 11:10:58 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2024-07-09 11:10:58 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2024-07-09 11:10:58 | INFO | fairseq_cli.train | num. shared model params: 13,088,768 (num. trained: 13,088,768)\n",
      "2024-07-09 11:10:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 133, in main\n",
      "    task.load_dataset(valid_sub_split, combine=False, epoch=1)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 338, in load_dataset\n",
      "    self.datasets[split] = load_langpair_dataset(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 81, in load_langpair_dataset\n",
      "    raise FileNotFoundError(\n",
      "FileNotFoundError: Dataset not found: valid (./drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess)\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
    "  ./drive/MyDrive/Dataset/bifi-dataset/round_0/data_paired/fairseq_preprocess \\\n",
    "  --source-lang bad --target-lang good \\\n",
    "  --arch transformer --share-all-embeddings \\\n",
    "  --encoder-layers 4 --decoder-layers 4 \\\n",
    "  --encoder-embed-dim 256 --decoder-embed-dim 256 \\\n",
    "  --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "  --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "  --encoder-normalize-before --decoder-normalize-before \\\n",
    "  --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \\\n",
    "  --weight-decay 0.0001 \\\n",
    "  --criterion label_smoothed_cross_entropy \\\n",
    "  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1 \\\n",
    "  --lr-scheduler inverse_sqrt --warmup-updates 400 --warmup-init-lr 0.0001 \\\n",
    "  --lr 0.001 \\\n",
    "  --max-tokens 13500 \\\n",
    "  --update-freq 2 \\\n",
    "  --max-epoch 2 --save-interval 1 --save-dir drive/MyDrive/Dataset/bifi-dataset/round_0/model-fixer \\\n",
    "  --label-smoothing 0.1 --save-interval-updates 10000 --num-workers 4 --memory-efficient-fp16 \\\n",
    "  2>&1 | tee -a drive/MyDrive/Dataset/bifi-dataset/round_0/model-fixer/train.log.txt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TnGpVGlJLt3u",
    "ExecuteTime": {
     "end_time": "2024-07-11T04:23:49.696680Z",
     "start_time": "2024-07-11T04:23:45.318910Z"
    }
   },
   "source": "!fairseq-train -h",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                     [--log-format {json,none,simple,tqdm}]\n",
      "                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n",
      "                     [--aim-run-hash AIM_RUN_HASH]\n",
      "                     [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                     [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                     [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                     [--memory-efficient-bf16] [--fp16]\n",
      "                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
      "                     [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                     [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                     [--on-cpu-convert-precision]\n",
      "                     [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                     [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                     [--amp-init-scale AMP_INIT_SCALE]\n",
      "                     [--amp-scale-window AMP_SCALE_WINDOW]\n",
      "                     [--user-dir USER_DIR]\n",
      "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                     [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                     [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                     [--tokenizer {moses,nltk,space}]\n",
      "                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                     [--task TASK] [--num-workers NUM_WORKERS]\n",
      "                     [--skip-invalid-size-inputs-valid-test]\n",
      "                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                     [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                     [--train-subset TRAIN_SUBSET]\n",
      "                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]\n",
      "                     [--ignore-unused-valid-subsets]\n",
      "                     [--validate-interval VALIDATE_INTERVAL]\n",
      "                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                     [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                     [--disable-validation]\n",
      "                     [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                     [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                     [--max-valid-steps MAX_VALID_STEPS]\n",
      "                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\n",
      "                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n",
      "                     [--grouped-shuffling]\n",
      "                     [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                     [--update-ordered-indices-seed]\n",
      "                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                     [--distributed-rank DISTRIBUTED_RANK]\n",
      "                     [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                     [--distributed-port DISTRIBUTED_PORT]\n",
      "                     [--device-id DEVICE_ID] [--distributed-no-spawn]\n",
      "                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                     [--ddp-comm-hook {none,fp16}]\n",
      "                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\n",
      "                     [--find-unused-parameters] [--gradient-as-bucket-view]\n",
      "                     [--fast-stat-sync]\n",
      "                     [--heartbeat-timeout HEARTBEAT_TIMEOUT]\n",
      "                     [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                     [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                     [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                     [--pipeline-model-parallel]\n",
      "                     [--pipeline-balance PIPELINE_BALANCE]\n",
      "                     [--pipeline-devices PIPELINE_DEVICES]\n",
      "                     [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                     [--pipeline-checkpoint {always,never,except_last}]\n",
      "                     [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                     [--fp32-reduce-scatter] [--cpu-offload]\n",
      "                     [--use-sharded-state] [--not-fsdp-flatten-parameters]\n",
      "                     [--arch ARCH] [--max-epoch MAX_EPOCH]\n",
      "                     [--max-update MAX_UPDATE]\n",
      "                     [--stop-time-hours STOP_TIME_HOURS]\n",
      "                     [--clip-norm CLIP_NORM] [--sentence-avg]\n",
      "                     [--update-freq UPDATE_FREQ] [--lr LR]\n",
      "                     [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n",
      "                     [--skip-remainder-batch] [--save-dir SAVE_DIR]\n",
      "                     [--restore-file RESTORE_FILE]\n",
      "                     [--continue-once CONTINUE_ONCE]\n",
      "                     [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                     [--reset-dataloader] [--reset-lr-scheduler]\n",
      "                     [--reset-meters] [--reset-optimizer]\n",
      "                     [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                     [--save-interval SAVE_INTERVAL]\n",
      "                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                     [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\n",
      "                     [--no-save] [--no-epoch-checkpoints]\n",
      "                     [--no-last-checkpoints] [--no-save-optimizer-state]\n",
      "                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                     [--load-checkpoint-on-all-dp-ranks]\n",
      "                     [--write-checkpoints-asynchronously] [--store-ema]\n",
      "                     [--ema-decay EMA_DECAY]\n",
      "                     [--ema-start-update EMA_START_UPDATE]\n",
      "                     [--ema-seed-model EMA_SEED_MODEL]\n",
      "                     [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --no-progress-bar     disable progress bar\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        log progress every N batches (when progress bar is\n",
      "                        disabled)\n",
      "  --log-format {json,none,simple,tqdm}\n",
      "                        log format to use\n",
      "  --log-file LOG_FILE   log file to copy metrics to.\n",
      "  --aim-repo AIM_REPO   path to Aim repository\n",
      "  --aim-run-hash AIM_RUN_HASH\n",
      "                        Aim run hash. If skipped, creates or continues run\n",
      "                        based on save_dir\n",
      "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
      "                        path to save logs for tensorboard, should match\n",
      "                        --logdir of running tensorboard (default: no\n",
      "                        tensorboard logging)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Weights and Biases project name to use for logging\n",
      "  --azureml-logging     Log scalars to AzureML context\n",
      "  --seed SEED           pseudo random number generator seed\n",
      "  --cpu                 use CPU instead of CUDA\n",
      "  --tpu                 use TPU instead of CUDA\n",
      "  --bf16                use bfloat16; implies --tpu\n",
      "  --memory-efficient-bf16\n",
      "                        use a memory-efficient version of BF16 training;\n",
      "                        implies --bf16\n",
      "  --fp16                use FP16\n",
      "  --memory-efficient-fp16\n",
      "                        use a memory-efficient version of FP16 training;\n",
      "                        implies --fp16\n",
      "  --fp16-no-flatten-grads\n",
      "                        don't flatten FP16 grads tensor\n",
      "  --fp16-init-scale FP16_INIT_SCALE\n",
      "                        default FP16 loss scale\n",
      "  --fp16-scale-window FP16_SCALE_WINDOW\n",
      "                        number of updates before increasing loss scale\n",
      "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
      "                        pct of updates that can overflow before decreasing the\n",
      "                        loss scale\n",
      "  --on-cpu-convert-precision\n",
      "                        if set, the floating point conversion to fp16/bf16\n",
      "                        runs on CPU. This reduces bus transfer time and GPU\n",
      "                        memory usage.\n",
      "  --min-loss-scale MIN_LOSS_SCALE\n",
      "                        minimum FP16/AMP loss scale, after which training is\n",
      "                        stopped\n",
      "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
      "                        threshold FP16 loss scale from below\n",
      "  --amp                 use automatic mixed precision\n",
      "  --amp-batch-retries AMP_BATCH_RETRIES\n",
      "                        number of retries of same batch after reducing loss\n",
      "                        scale with AMP\n",
      "  --amp-init-scale AMP_INIT_SCALE\n",
      "                        default AMP loss scale\n",
      "  --amp-scale-window AMP_SCALE_WINDOW\n",
      "                        number of updates before increasing AMP loss scale\n",
      "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
      "                        (tasks and/or architectures)\n",
      "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
      "                        how often to clear the PyTorch CUDA cache (0 to\n",
      "                        disable)\n",
      "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
      "                        number of bytes reserved for gathering stats from\n",
      "                        workers\n",
      "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
      "                        total number of GPUs to parallelize model over\n",
      "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
      "                        path to quantization config file\n",
      "  --profile             enable autograd profiler emit_nvtx\n",
      "  --reset-logging       when using Hydra, reset the logging at the beginning\n",
      "                        of training\n",
      "  --suppress-crashes    suppress crashes when training with the hydra_train\n",
      "                        entry point so that the main method can return a value\n",
      "                        (useful for sweeps)\n",
      "  --use-plasma-view     Store indices and sizes in shared memory\n",
      "  --plasma-path PLASMA_PATH\n",
      "                        path to run plasma_store, defaults to /tmp/plasma.\n",
      "                        Paths outside /tmp tend to fail.\n",
      "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
      "  --tokenizer {moses,nltk,space}\n",
      "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
      "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
      "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
      "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
      "  --task TASK           task\n",
      "\n",
      "dataset_data_loading:\n",
      "  --num-workers NUM_WORKERS\n",
      "                        how many subprocesses to use for data loading\n",
      "  --skip-invalid-size-inputs-valid-test\n",
      "                        ignore too long or too short lines in valid and test\n",
      "                        set\n",
      "  --max-tokens MAX_TOKENS\n",
      "                        maximum number of tokens in a batch\n",
      "  --batch-size BATCH_SIZE, --max-sentences BATCH_SIZE\n",
      "                        number of examples in a batch\n",
      "  --required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE\n",
      "                        batch size will be a multiplier of this value\n",
      "  --required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE\n",
      "                        maximum sequence length in batch will be a multiplier\n",
      "                        of this value\n",
      "  --dataset-impl {raw,lazy,cached,mmap,fasta,huffman}\n",
      "                        output dataset implementation\n",
      "  --data-buffer-size DATA_BUFFER_SIZE\n",
      "                        Number of batches to preload\n",
      "  --train-subset TRAIN_SUBSET\n",
      "                        data subset to use for training (e.g. train, valid,\n",
      "                        test)\n",
      "  --valid-subset VALID_SUBSET\n",
      "                        comma separated list of data subsets to use for\n",
      "                        validation (e.g. train, valid, test)\n",
      "  --combine-valid-subsets, --combine-val\n",
      "                        comma separated list of data subsets to use for\n",
      "                        validation (e.g. train, valid, test)\n",
      "  --ignore-unused-valid-subsets\n",
      "                        do not raise error if valid subsets are ignored\n",
      "  --validate-interval VALIDATE_INTERVAL\n",
      "                        validate every N epochs\n",
      "  --validate-interval-updates VALIDATE_INTERVAL_UPDATES\n",
      "                        validate every N updates\n",
      "  --validate-after-updates VALIDATE_AFTER_UPDATES\n",
      "                        dont validate until reaching this many updates\n",
      "  --fixed-validation-seed FIXED_VALIDATION_SEED\n",
      "                        specified random seed for validation\n",
      "  --disable-validation  disable validation\n",
      "  --max-tokens-valid MAX_TOKENS_VALID\n",
      "                        maximum number of tokens in a validation batch\n",
      "                        (defaults to --max-tokens)\n",
      "  --batch-size-valid BATCH_SIZE_VALID, --max-sentences-valid BATCH_SIZE_VALID\n",
      "                        batch size of the validation batch (defaults to\n",
      "                        --batch-size)\n",
      "  --max-valid-steps MAX_VALID_STEPS, --nval MAX_VALID_STEPS\n",
      "                        How many batches to evaluate\n",
      "  --curriculum CURRICULUM\n",
      "                        don't shuffle batches for first N epochs\n",
      "  --gen-subset GEN_SUBSET\n",
      "                        data subset to generate (train, valid, test)\n",
      "  --num-shards NUM_SHARDS\n",
      "                        shard generation over N shards\n",
      "  --shard-id SHARD_ID   id of the shard to generate (id < num_shards)\n",
      "  --grouped-shuffling   shuffle batches in groups of num_shards to enable\n",
      "                        similar sequence lengths on each GPU worker when\n",
      "                        batches are sorted by length\n",
      "  --update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR\n",
      "                        if true then prevents the reuse the epoch batch\n",
      "                        iterator by setting can_reuse_epoch_itr to false,\n",
      "                        defaults to --grouped-shuffling )\n",
      "  --update-ordered-indices-seed\n",
      "                        if true then increment seed with epoch for getting\n",
      "                        batch iterators, defautls to False.\n",
      "\n",
      "distributed_training:\n",
      "  --distributed-world-size DISTRIBUTED_WORLD_SIZE\n",
      "                        total number of GPUs across all nodes (default: all\n",
      "                        visible GPUs)\n",
      "  --distributed-num-procs DISTRIBUTED_NUM_PROCS\n",
      "                        total number of processes to fork (default: all\n",
      "                        visible GPUs)\n",
      "  --distributed-rank DISTRIBUTED_RANK\n",
      "                        rank of the current worker\n",
      "  --distributed-backend DISTRIBUTED_BACKEND\n",
      "                        distributed backend\n",
      "  --distributed-init-method DISTRIBUTED_INIT_METHOD\n",
      "                        typically tcp://hostname:port that will be used to\n",
      "                        establish initial connetion\n",
      "  --distributed-port DISTRIBUTED_PORT\n",
      "                        port number (not required if using --distributed-init-\n",
      "                        method)\n",
      "  --device-id DEVICE_ID, --local_rank DEVICE_ID\n",
      "                        which GPU to use (by default looks for $LOCAL_RANK,\n",
      "                        usually configured automatically)\n",
      "  --distributed-no-spawn\n",
      "                        do not spawn multiple processes even if multiple GPUs\n",
      "                        are visible\n",
      "  --ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}\n",
      "                        DistributedDataParallel backend\n",
      "  --ddp-comm-hook {none,fp16}\n",
      "                        communication hook\n",
      "  --bucket-cap-mb BUCKET_CAP_MB\n",
      "                        bucket size for reduction\n",
      "  --fix-batches-to-gpus\n",
      "                        don't shuffle batches between GPUs; this reduces\n",
      "                        overall randomness and may affect precision but avoids\n",
      "                        the cost of re-reading the data\n",
      "  --find-unused-parameters\n",
      "                        disable unused parameter detection (not applicable to\n",
      "                        --ddp-backend=legacy_ddp)\n",
      "  --gradient-as-bucket-view\n",
      "                        when set to True, gradients will be views pointing to\n",
      "                        different offsets of allreduce communication buckets.\n",
      "                        This can reduce peak memory usage, where the saved\n",
      "                        memory size will be equal to the total gradients size.\n",
      "                        --gradient-as-bucket-view=gradient_as_bucket_view)\n",
      "  --fast-stat-sync      [deprecated] this is now defined per Criterion\n",
      "  --heartbeat-timeout HEARTBEAT_TIMEOUT\n",
      "                        kill the job if no progress is made in N seconds; set\n",
      "                        to -1 to disable\n",
      "  --broadcast-buffers   Copy non-trainable parameters between GPUs, such as\n",
      "                        batchnorm population statistics\n",
      "  --slowmo-momentum SLOWMO_MOMENTUM\n",
      "                        SlowMo momentum term; by default use 0.0 for 16 GPUs,\n",
      "                        0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs\n",
      "  --slowmo-base-algorithm SLOWMO_BASE_ALGORITHM\n",
      "                        Base algorithm. Either 'localsgd' or 'sgp'. Please\n",
      "                        refer to the documentation of 'slowmo_base_algorithm'\n",
      "                        parameter in https://fairscale.readthedocs.io/en/lates\n",
      "                        t/api/experimental/nn/slowmo_ddp.html for more details\n",
      "  --localsgd-frequency LOCALSGD_FREQUENCY\n",
      "                        Local SGD allreduce frequency\n",
      "  --nprocs-per-node NPROCS_PER_NODE\n",
      "                        number of GPUs in each node. An allreduce operation\n",
      "                        across GPUs in a node is very fast. Hence, we do\n",
      "                        allreduce across GPUs in a node, and gossip across\n",
      "                        different nodes\n",
      "  --pipeline-model-parallel\n",
      "                        if set, use pipeline model parallelism across GPUs\n",
      "  --pipeline-balance PIPELINE_BALANCE\n",
      "                        partition the model into N_K pieces, where each piece\n",
      "                        contains N_i layers. The sum(args.pipeline_balance)\n",
      "                        should equal the total number of layers in the model\n",
      "  --pipeline-devices PIPELINE_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-balance\n",
      "                        argument\n",
      "  --pipeline-chunks PIPELINE_CHUNKS\n",
      "                        microbatch count for pipeline model parallelism\n",
      "  --pipeline-encoder-balance PIPELINE_ENCODER_BALANCE\n",
      "                        partition the pipeline parallel encoder into N_K\n",
      "                        pieces, where each piece contains N_i layers. The\n",
      "                        sum(args.pipeline_encoder_balance) should equal the\n",
      "                        total number of encoder layers in the model\n",
      "  --pipeline-encoder-devices PIPELINE_ENCODER_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-\n",
      "                        encoder-balance argument\n",
      "  --pipeline-decoder-balance PIPELINE_DECODER_BALANCE\n",
      "                        partition the pipeline parallel decoder into N_K\n",
      "                        pieces, where each piece contains N_i layers. The\n",
      "                        sum(args.pipeline_decoder_balance) should equal the\n",
      "                        total number of decoder layers in the model\n",
      "  --pipeline-decoder-devices PIPELINE_DECODER_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-\n",
      "                        decoder-balance argument\n",
      "  --pipeline-checkpoint {always,never,except_last}\n",
      "                        checkpointing mode for pipeline model parallelism\n",
      "  --zero-sharding {none,os}\n",
      "                        ZeRO sharding\n",
      "  --no-reshard-after-forward\n",
      "                        don't reshard parameters after forward pass\n",
      "  --fp32-reduce-scatter\n",
      "                        reduce-scatter grads in FP32\n",
      "  --cpu-offload         offload FP32 params to CPU\n",
      "  --use-sharded-state   use sharded checkpoint files\n",
      "  --not-fsdp-flatten-parameters\n",
      "                        not flatten parameter param for fsdp\n",
      "\n",
      "Model configuration:\n",
      "  --arch ARCH, -a ARCH  model architecture\n",
      "\n",
      "optimization:\n",
      "  --max-epoch MAX_EPOCH\n",
      "                        force stop training at specified epoch\n",
      "  --max-update MAX_UPDATE\n",
      "                        force stop training at specified update\n",
      "  --stop-time-hours STOP_TIME_HOURS\n",
      "                        force stop training after specified cumulative time\n",
      "                        (if >0)\n",
      "  --clip-norm CLIP_NORM\n",
      "                        clip threshold of gradients\n",
      "  --sentence-avg        normalize gradients by the number of sentences in a\n",
      "                        batch (default is to normalize by number of tokens)\n",
      "  --update-freq UPDATE_FREQ\n",
      "                        update parameters every N_i batches, when in epoch i\n",
      "  --lr LR               learning rate for the first N epochs; all epochs >N\n",
      "                        using LR_N (note: this may be interpreted differently\n",
      "                        depending on --lr-scheduler)\n",
      "  --stop-min-lr STOP_MIN_LR\n",
      "                        stop training when the learning rate reaches this\n",
      "                        minimum\n",
      "  --use-bmuf            specify global optimizer for syncing models on\n",
      "                        different GPUs/shards\n",
      "  --skip-remainder-batch\n",
      "                        if set, include the last (partial) batch of each epoch\n",
      "                        in training (default is to skip it).\n",
      "\n",
      "checkpoint:\n",
      "  --save-dir SAVE_DIR   path to save checkpoints\n",
      "  --restore-file RESTORE_FILE\n",
      "                        filename from which to load checkpoint (default:\n",
      "                        <save-dir>/checkpoint_last.pt\n",
      "  --continue-once CONTINUE_ONCE\n",
      "                        continues from this checkpoint, unless a checkpoint\n",
      "                        indicated in 'restore_file' option is present\n",
      "  --finetune-from-model FINETUNE_FROM_MODEL\n",
      "                        finetune from a pretrained model; note that meters and\n",
      "                        lr scheduler will be reset\n",
      "  --reset-dataloader    if set, does not reload dataloader state from the\n",
      "                        checkpoint\n",
      "  --reset-lr-scheduler  if set, does not load lr scheduler state from the\n",
      "                        checkpoint\n",
      "  --reset-meters        if set, does not load meters from the checkpoint\n",
      "  --reset-optimizer     if set, does not load optimizer state from the\n",
      "                        checkpoint\n",
      "  --optimizer-overrides OPTIMIZER_OVERRIDES\n",
      "                        a dictionary used to override optimizer args when\n",
      "                        loading a checkpoint\n",
      "  --save-interval SAVE_INTERVAL\n",
      "                        save a checkpoint every N epochs\n",
      "  --save-interval-updates SAVE_INTERVAL_UPDATES\n",
      "                        save a checkpoint (and validate) every N updates\n",
      "  --keep-interval-updates KEEP_INTERVAL_UPDATES\n",
      "                        keep the last N checkpoints saved with --save-\n",
      "                        interval-updates\n",
      "  --keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN\n",
      "                        when used with --keep-interval-updates, skips deleting\n",
      "                        any checkpoints with update X where X %\n",
      "                        keep_interval_updates_pattern == 0\n",
      "  --keep-last-epochs KEEP_LAST_EPOCHS\n",
      "                        keep last N epoch checkpoints\n",
      "  --keep-best-checkpoints KEEP_BEST_CHECKPOINTS\n",
      "                        keep best N checkpoints based on scores\n",
      "  --no-save             don't save models or checkpoints\n",
      "  --no-epoch-checkpoints\n",
      "                        only store last and best checkpoints\n",
      "  --no-last-checkpoints\n",
      "                        don't store last checkpoints\n",
      "  --no-save-optimizer-state\n",
      "                        don't save optimizer-state as part of checkpoint\n",
      "  --best-checkpoint-metric BEST_CHECKPOINT_METRIC\n",
      "                        metric to use for saving \"best\" checkpoints\n",
      "  --maximize-best-checkpoint-metric\n",
      "                        select the largest metric value for saving \"best\"\n",
      "                        checkpoints\n",
      "  --patience PATIENCE   early stop training if valid performance doesn't\n",
      "                        improve for N consecutive validation runs; note that\n",
      "                        this is influenced by --validate-interval\n",
      "  --checkpoint-suffix CHECKPOINT_SUFFIX\n",
      "                        suffix to add to the checkpoint file name\n",
      "  --checkpoint-shard-count CHECKPOINT_SHARD_COUNT\n",
      "                        Number of shards containing the checkpoint - if the\n",
      "                        checkpoint is over 300GB, it is preferable to split it\n",
      "                        into shards to prevent OOM on CPU while loading the\n",
      "                        checkpoint\n",
      "  --load-checkpoint-on-all-dp-ranks\n",
      "                        load checkpoints on all data parallel devices\n",
      "                        (default: only load on rank 0 and broadcast to other\n",
      "                        devices)\n",
      "  --write-checkpoints-asynchronously, --save-async\n",
      "                        Write checkpoints asynchronously in a separate thread.\n",
      "                        NOTE: This feature is currently being tested.\n",
      "\n",
      "EMA configuration:\n",
      "  --store-ema\n",
      "  --ema-decay EMA_DECAY\n",
      "                        decay for exponential moving average model\n",
      "  --ema-start-update EMA_START_UPDATE\n",
      "                        start EMA update after this many model updates\n",
      "  --ema-seed-model EMA_SEED_MODEL\n",
      "                        Seed to load EMA model from. Used to load EMA model\n",
      "                        separately from the actual model.\n",
      "  --ema-update-freq EMA_UPDATE_FREQ\n",
      "                        Do EMA update every this many model updates\n",
      "  --ema-fp32            If true, store EMA model in fp32 even if model is in\n",
      "                        fp16\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!fairseq-train \\\n",
    "    data/round0/data_paired/fairseq_preprocess \\\n",
    "    --source-lang bad --target-lang good \\\n",
    "                                --arch transformer --share-all-embeddings \\\n",
    "                                       --encoder-layers 4 --decoder-layers 4 \\\n",
    "                                                                           --encoder-embed-dim 256 --decoder-embed-dim 256 \\\n",
    "                                                                                                                       --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024 \\\n",
    "                                                                                                                                                                            --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
    "                                                                                                                                                                                                                                  --encoder-normalize-before --decoder-normalize-before \\\n",
    "                                                                                                                                                                                                                                  --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2 \\\n",
    "                                                                                                                                                                                                                                                                                       --weight-decay 0.0001 \\\n",
    "                                                                                                                                                                                                                                                                                                      --criterion label_smoothed_cross_entropy \\\n",
    "                                                                                                                                                                                                                                                                                                                  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1 \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                          --lr-scheduler inverse_sqrt --warmup-updates 400 --warmup-init-lr 0.0001 \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                            --lr 0.001 \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                 --max-tokens 13500 \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                              --update-freq 2 \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                            --max-epoch 2 --save-interval 1 --save-dir data/round0/model-fixer \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       --label-smoothing 0.1 --save-interval-updates 10000 \\\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     --num-workers 4 --memory-efficient-fp16 \\\n",
    "    2>&1 | tee -a data/round0/model-fixer/train.log.txt "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
