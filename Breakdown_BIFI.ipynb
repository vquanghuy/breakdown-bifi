{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMh9r11F0BD9"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxK14PmC9Yzz"
   },
   "source": [
    "We should check if we're on the Colab and do additional setup\n",
    "- Install `fairseq`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ngsjzyFax4KL",
    "outputId": "c51fe7f1-6d57-4234-93d4-5f24237a98f2",
    "ExecuteTime": {
     "end_time": "2024-07-18T08:36:16.456957Z",
     "start_time": "2024-07-18T08:36:16.450041Z"
    }
   },
   "source": [
    "from IPython.core import getipython\n",
    "\n",
    "is_colab = 'google.colab' in str(getipython.get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    !git clone https://github.com/vquanghuy/breakdown-bifi\n",
    "    !cp -r breakdown-bifi/utils .\n",
    "    !pip install fairseq editdistance\n",
    "else:\n",
    "  print(\"Notebook is not on Colab. Fairseq installation not attempted.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is not on Colab. Fairseq installation not attempted.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rAuWy859T45",
    "outputId": "12d863f7-6d1b-48dc-b58d-5d9d3e59c903",
    "ExecuteTime": {
     "end_time": "2024-07-18T08:36:19.984413Z",
     "start_time": "2024-07-18T08:36:18.029499Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# Check PyTorch version\n",
    "import torch\n",
    "print('Torch', torch.__version__)\n",
    "\n",
    "import fairseq\n",
    "print('fairseq', fairseq.__version__)\n",
    "\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json, os, re\n",
    "import token\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from copy import deepcopy\n",
    "import editdistance\n",
    "\n",
    "sys.path.insert(0, 'utils')  # Replace with the actual path"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 15:36:19 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq 0.12.2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNBdpnZp_bQt"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0q3s-dVP_L00",
    "ExecuteTime": {
     "end_time": "2024-07-18T08:36:26.062123Z",
     "start_time": "2024-07-18T08:36:26.059541Z"
    }
   },
   "source": [
    "DATA_DIR = 'drive/MyDrive/Dataset/bifi-dataset' if is_colab else 'data'\n",
    "os.environ[\"DATA_DIR\"] = DATA_DIR"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38ITJmmMBrcQ"
   },
   "source": [
    "# Supported functions"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, OrderedDict\n",
    "import hashlib\n",
    "\n",
    "from utils.code_error_checker import check_paren_error, check_ast_error\n",
    "from utils.code_utils import preprocess_unk, code_toks_to_code_string, get_diff_metric\n",
    "from utils.fairseq_utils import parse_fairseq_preds, fairseq_preprocess, fairseq_generate, fairseq_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "A9M8VHuFMJa8"
   },
   "cell_type": "markdown",
   "source": [
    "## eval_fixer"
   ]
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "pUFo3AZtMJa8",
    "outputId": "1b10af7f-75dd-401f-9d52-ba95272aab74",
    "ExecuteTime": {
     "end_time": "2024-07-18T08:36:45.504074Z",
     "start_time": "2024-07-18T08:36:45.479923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_one_pred_obj(pred_obj):\n",
    "    # Deal with UNK\n",
    "    _, unk_dict = preprocess_unk(pred_obj['code_toks_raw'])\n",
    "    anonymize_dict = pred_obj['anonymize_dict']\n",
    "    if anonymize_dict is None:\n",
    "        anonymize_dict = {}\n",
    "    anonymize_dict['<unk>'] = unk_dict\n",
    "    anonymize_dict['<STRING>'] = []\n",
    "    anonymize_dict['<COMMENT>'] = []\n",
    "    #\n",
    "    src = pred_obj['src'] #this is tok_format i.e. ' '.join(code_toks)\n",
    "    src_code  = code_toks_to_code_string(src, anonymize_dict) #this is string_format\n",
    "    ret_obj = {'progid': pred_obj['progid'],\n",
    "               'orig_err_obj': pred_obj['orig_err_obj'],\n",
    "               'anonymize_dict': pred_obj['anonymize_dict']\n",
    "               }\n",
    "    ret_obj['src']  = {'tok_format': src, 'string_format': src_code}\n",
    "    #Get string_format from predicted code toks\n",
    "    ret_obj['pred'] = []\n",
    "    for pred in pred_obj['pred']:\n",
    "        pred_code = code_toks_to_code_string(pred, anonymize_dict) #this is string_format\n",
    "        orig_err_obj = pred_obj['orig_err_obj']\n",
    "        if orig_err_obj['msg'] == 'unbalanced (){}[]':\n",
    "            #NOTE: `pred` is tok_format i.e. ' '.join(code_toks)\n",
    "            res = check_paren_error(pred.split())\n",
    "        else:\n",
    "            res = check_ast_error(pred_code)\n",
    "        diff_metric = get_diff_metric(src, pred)\n",
    "        ret_obj['pred'].append({'tok_format': pred,\n",
    "                                'string_format': pred_code,\n",
    "                                'err_obj': res,\n",
    "                                'diff_metric': diff_metric})\n",
    "    return ret_obj\n",
    "\n",
    "def eval_one_split(pred_dir_prefix, split, pred_fname, n_workers=80):\n",
    "    pred_dir   = f'{pred_dir_prefix}{split}'\n",
    "    pred_path = Path(f'{pred_dir}/{pred_fname}')\n",
    "    preds = parse_fairseq_preds(str(pred_path))\n",
    "    #load progids\n",
    "    data_dir = DATA_DIR\n",
    "    progids = [l.strip() for l in open(f'{data_dir}/orig_bad_code/orig.{split}.id')]\n",
    "    assert len(preds) == len(progids)\n",
    "    #load original err_obj\n",
    "    bads = json.load(open(f'{data_dir}/orig_bad_code/orig.bad.json'))\n",
    "    for j in range(len(preds)):\n",
    "        progid = progids[j]\n",
    "        preds[j]['progid'] = progid\n",
    "        preds[j]['orig_err_obj'] = bads[progid]['err_obj']\n",
    "        code_toks_raw = bads[progid]['code_toks_joined'].split()\n",
    "        anonymize_dict = bads[progid]['anonymize_dict']\n",
    "        if 'window_span' in bads[progid]:\n",
    "            ws = bads[progid]['window_span']\n",
    "            code_toks_raw = code_toks_raw[ws[0]:ws[1]]\n",
    "            anonymize_dict = None\n",
    "        preds[j]['code_toks_raw'] = code_toks_raw\n",
    "        preds[j]['anonymize_dict'] = anonymize_dict\n",
    "    #\n",
    "    print ('len(preds)', len(preds))\n",
    "    # with Pool(n_workers) as p:\n",
    "    #     res = list(tqdm(p.imap(eval_one_pred_obj, preds), total=len(preds)))\n",
    "    res = list(tqdm(map(eval_one_pred_obj, preds)))  # or list(tqdm([eval_one_pred_obj(pred) for pred in preds]))\n",
    "\n",
    "    '''\n",
    "      res: list of {'progid': , 'orig_err_obj': , 'anonymize_dict': ,\n",
    "                    'src': {'tok_format': , 'string_format': },\n",
    "                    'pred': {'tok_format':, 'string_format':, 'err_obj': }\n",
    "                    }\n",
    "    '''\n",
    "    with open(f'{pred_path.parent}/{pred_path.stem}.evaluated.json', 'w') as f:\n",
    "        json.dump(res, f, indent=2)\n",
    "\n",
    "def get_test_result(pred_dir_prefix, pred_fname):\n",
    "    #\n",
    "    def collate_eval():\n",
    "        success  = []; denom = 0\n",
    "        success_by_group = defaultdict(list); denom_by_group = defaultdict(int)\n",
    "        agg_obj = {}\n",
    "        for split in {3,4}: #heldout test set\n",
    "            print ('split', split)\n",
    "            pred_dir   = Path(f'{pred_dir_prefix}{split}')\n",
    "            pred_path  = pred_dir/pred_fname\n",
    "            pred_eval_path = f'{pred_path.parent}/{pred_path.stem}.evaluated.json'\n",
    "            eval_objs = json.load(open(pred_eval_path))\n",
    "            for eval_obj in eval_objs:\n",
    "                progid = eval_obj['progid']\n",
    "                orig_err_type = eval_obj['orig_err_obj']['msg']\n",
    "                if 'indent' in orig_err_type:\n",
    "                    orig_err_type = 'indentation error'\n",
    "                denom += 1\n",
    "                denom_by_group[orig_err_type] += 1\n",
    "                for k, pred_obj in enumerate(eval_obj['pred']):\n",
    "                    pred_err_obj = pred_obj['err_obj']\n",
    "                    diff_metric  = pred_obj['diff_metric']\n",
    "                    if (pred_err_obj == 0) and (0 < diff_metric <= 4):\n",
    "                        name = '{:02d}-{}-{:03d}'.format(split, progid, k)\n",
    "                        success.append(name)\n",
    "                        success_by_group[orig_err_type].append(name)\n",
    "        return success, denom, success_by_group, denom_by_group\n",
    "    #\n",
    "    def print_stats(name_list, _denom):\n",
    "        top1 = set()\n",
    "        for name in name_list:\n",
    "            split, progid, k = name.split('-')\n",
    "            if int(split) in {3,4}: #test set\n",
    "                if int(k)==0:\n",
    "                    top1.add(f'{split}-{progid}')\n",
    "        acc = len(top1)/float(_denom)*100\n",
    "        print ('   acc: {} ({:.1f}%) | denom {}'.format(len(top1), acc, _denom))\n",
    "        return acc\n",
    "    #\n",
    "    success, denom, success_by_group, denom_by_group = collate_eval()\n",
    "    acc_dict = {}\n",
    "    print ('Total'); acc = print_stats(success, denom); acc_dict['total'] = acc\n",
    "    print ('-'*50)\n",
    "    for err_type in success_by_group:\n",
    "        print (f'{err_type.capitalize()}')\n",
    "        acc = print_stats(success_by_group[err_type], denom_by_group[err_type])\n",
    "        acc_dict[err_type] = acc\n",
    "    json.dump(acc_dict, open(Path(pred_dir_prefix).parent/'stats.json', 'w'), indent=2)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## generate_paired_data_from_fixer\n",
    "Using *critic* to verify"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#BIFI version - uses critic to verify\n",
    "def generate_paired_data_from_fixer_preds_for_BIFI(pred_dir_prefix, pred_fname, out_dir):\n",
    "    #Get new paired data\n",
    "    train_data = {'good': [], 'bad': [], 'id': []}\n",
    "    for split in {0,1,2}: #available for training\n",
    "        print ('split', split)\n",
    "        pred_dir   = Path(f'{pred_dir_prefix}{split}')\n",
    "        pred_path  = pred_dir/pred_fname\n",
    "        pred_eval_path = f'{pred_path.parent}/{pred_path.stem}.evaluated.json'\n",
    "        eval_objs = json.load(open(pred_eval_path))\n",
    "        for eval_obj in eval_objs:\n",
    "            progid = eval_obj['progid']\n",
    "            for k, pred_obj in enumerate(eval_obj['pred']):\n",
    "                pred_err_obj = pred_obj['err_obj']\n",
    "                diff_metric  = pred_obj['diff_metric']\n",
    "                if (pred_err_obj == 0) and (0 < diff_metric <= 4):\n",
    "                    name = '{:02d}-{}-{:03d}'.format(split, progid, k)\n",
    "                    src  = eval_obj['src']['tok_format'].strip()\n",
    "                    pred = pred_obj['tok_format'].strip()\n",
    "                    train_data['id'  ].append(name)\n",
    "                    train_data['good'].append(pred)\n",
    "                    train_data['bad' ].append(src)\n",
    "    assert len(train_data['good']) == len(train_data['bad']) == len(train_data['id'])\n",
    "    new_data_size = len(train_data['id'])\n",
    "    print ('#new_data', new_data_size)\n",
    "    os.system(f'mkdir -p {out_dir}_pure')\n",
    "    with open(f'{out_dir}_pure/train.id', 'w') as fid, \\\n",
    "            open(f'{out_dir}_pure/train.good', 'w') as fgood, \\\n",
    "            open(f'{out_dir}_pure/train.bad', 'w') as fbad:\n",
    "        for _idx in tqdm(range(new_data_size)):\n",
    "            fid.write(train_data['id'][_idx] +'\\n')\n",
    "            fgood.write(train_data['good'][_idx] +'\\n')\n",
    "            fbad.write(train_data['bad'][_idx] +'\\n')\n",
    "    idxs_newdata = list(range(new_data_size))\n",
    "    #\n",
    "    #Merge with round0 paired data\n",
    "    print ('loading round0 data')\n",
    "    train_data_0 = {'good': [], 'bad': [], 'id': []}\n",
    "    train_data_0['bad']  = [line.strip() for line in tqdm(open('data/round0/data_paired/train.bad'))]\n",
    "    train_data_0['good'] = [line.strip() for line in tqdm(open('data/round0/data_paired/train.good'))]\n",
    "    train_data_0['id']   = [line.strip() for line in tqdm(open('data/round0/data_paired/train.id'))]\n",
    "    idxs_0 = list(range(len(train_data_0['id'])))\n",
    "    seed = (111 + int(hashlib.md5(str(out_dir).encode()).hexdigest(), 16)) % (2**31)\n",
    "    print ('seed', seed)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idxs_0); np.random.shuffle(idxs_0)\n",
    "    total_size = 30_000_000\n",
    "    _0_data_repeats  = (total_size//3)//len(idxs_0) +1\n",
    "    new_data_repeats = (total_size*2//3)//new_data_size +1\n",
    "    idxs_0 = (idxs_0 * _0_data_repeats)[:total_size//3]\n",
    "    idxs_newdata = idxs_newdata * new_data_repeats\n",
    "    print ('combining all data')\n",
    "    idxs = [f'0_{i}' for i in idxs_0] + [f'new_{i}' for i in idxs_newdata]\n",
    "    np.random.shuffle(idxs); np.random.shuffle(idxs)\n",
    "    #\n",
    "    #Write out data\n",
    "    os.system(f'mkdir -p {out_dir}')\n",
    "    print ('writing out data')\n",
    "    with open(f'{out_dir}/train.id', 'w') as fid, \\\n",
    "            open(f'{out_dir}/train.good', 'w') as fgood, \\\n",
    "            open(f'{out_dir}/train.bad', 'w') as fbad:\n",
    "        for idx in tqdm(idxs):\n",
    "            _prefix, _idx = idx.split('_')\n",
    "            _idx = int(_idx)\n",
    "            if _prefix == '0':\n",
    "                fid.write(train_data_0['id'][_idx] +'\\n')\n",
    "                fgood.write(train_data_0['good'][_idx] +'\\n')\n",
    "                fbad.write(train_data_0['bad'][_idx] +'\\n')\n",
    "            else:\n",
    "                fid.write(train_data['id'][_idx] +'\\n')\n",
    "                fgood.write(train_data['good'][_idx] +'\\n')\n",
    "                fbad.write(train_data['bad'][_idx] +'\\n')\n",
    "    os.system('cp {} {}'.format('data/round0/data_paired/dev.bad', out_dir))\n",
    "    os.system('cp {} {}'.format('data/round0/data_paired/dev.good', out_dir))\n",
    "    os.system('cp {} {}'.format('data/round0/data_paired/dev.id', out_dir))\n",
    "    print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPrCdDrwB14d"
   },
   "source": [
    "# Round 0\n",
    "\n",
    "This round is mainly focus on training and evaluate `fixer`"
   ]
  },
  {
   "metadata": {
    "id": "rCf3QIDlMJa8"
   },
   "cell_type": "markdown",
   "source": [
    "## Round variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SWzdVzcktvpx",
    "outputId": "3bc8721b-c3d5-4fa5-8f7b-e4d59321d0e8",
    "ExecuteTime": {
     "end_time": "2024-07-18T08:37:21.000517Z",
     "start_time": "2024-07-18T08:37:20.981876Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(DATA_DIR)\n",
    "round_dir = data_dir/'round_0'\n",
    "\n",
    "# Preprocess and train\n",
    "data_paired_dir = round_dir/'small_data_paired' # Force to use smaller dataset - initial is data_paired\n",
    "fairseq_dir = data_paired_dir/'fairseq_preprocess'\n",
    "\n",
    "# Run fixer\n",
    "model_dir  = round_dir/'model-fixer'\n",
    "model_path = model_dir/'checkpoint_best.pt'\n",
    "destdir_root = round_dir/'orig_bad'\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "# Evaluate\n",
    "pred_dir_root = round_dir/'orig_bad'\n",
    "pred_dir_prefix = str(pred_dir_root/'fairseq_preprocess__orig_bad.')\n",
    "pred_fname  = 'model-fixer.pred.txt'"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "id": "GXMJ3QbVMJa9"
   },
   "cell_type": "markdown",
   "source": [
    "## Cleanup data\n",
    "\n",
    "Use with caution"
   ]
  },
  {
   "metadata": {
    "id": "ZaLbDBMbMJa9"
   },
   "cell_type": "markdown",
   "source": [
    "### Remove train's preprocessed data"
   ]
  },
  {
   "metadata": {
    "id": "XWmd5E8TMJa9"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shutil.rmtree(str(fairseq_dir))"
   ]
  },
  {
   "metadata": {
    "id": "wg1fOgQ8MJa9"
   },
   "cell_type": "markdown",
   "source": [
    "### Remove fixer preprocessed data\n",
    "\n",
    "It's used while running the fixer, it is required to have preprocessed data from bad code"
   ]
  },
  {
   "metadata": {
    "id": "mHTyuJ9AMJa9",
    "ExecuteTime": {
     "end_time": "2024-07-13T09:00:19.411082Z",
     "start_time": "2024-07-13T09:00:19.270268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shutil.rmtree(str(destdir_root))"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "id": "eQrt1ihwMJa9"
   },
   "cell_type": "markdown",
   "source": [
    "## Reduce data\n",
    "\n",
    "Since the input data is huge, this step is involved to reduce the size of the input, only keep **1 million record for training**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U1F1hxikSXG7",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:11:26.075341Z",
     "start_time": "2024-07-11T15:11:23.003372Z"
    }
   },
   "source": [
    "# Take 1m lines as sample\n",
    "from itertools import islice\n",
    "\n",
    "original_data_paired_dir = round_dir/'data_paired'\n",
    "train_sliced_lines = 1000000\n",
    "dev_sliced_lines = train_sliced_lines / 100\n",
    "\n",
    "data_paired_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare train.good and train.bad\n",
    "with open(str(original_data_paired_dir/'train.good'), 'r', encoding='utf-8') as infile, \\\n",
    "    open(str(data_paired_dir/'train.good'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n",
    "\n",
    "with open(str(original_data_paired_dir/'train.bad'), 'r', encoding=\"utf-8\") as infile, \\\n",
    "    open(str(data_paired_dir/'train.bad'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n",
    "\n",
    "# Prepare dev.good and dev.bad\n",
    "with open(str(original_data_paired_dir/'dev.good'), 'r', encoding=\"utf-8\") as infile, \\\n",
    "    open(str(data_paired_dir/'dev.good'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n",
    "\n",
    "with open(str(original_data_paired_dir/'dev.bad'), 'r', encoding=\"utf-8\") as infile, \\\n",
    "    open(str(data_paired_dir/'dev.bad'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkKlR8UWGSJR"
   },
   "source": [
    "## Preprocess data for training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZs7SU74DRw6",
    "outputId": "a94d70a9-fa90-4194-aea5-9e02f3707442",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:52:02.567485Z",
     "start_time": "2024-07-11T15:48:34.270167Z"
    }
   },
   "source": [
    "# Perform preprocess data\n",
    "fairseq_preprocess(src='bad', tgt='good', workers=20,\n",
    "                      destdir  = str(data_paired_dir/'fairseq_preprocess'),\n",
    "                      trainpref= str(data_paired_dir/'train'),\n",
    "                      validpref= str(data_paired_dir/'dev'),\n",
    "                      srcdict  = str(data_dir/'token_vocab.txt') )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-preprocess --source-lang bad --destdir data\\round_0\\small_data_paired\\fairseq_preprocess             --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 --target-lang good --trainpref data\\round_0\\small_data_paired\\train --validpref data\\round_0\\small_data_paired\\dev --srcdict data\\token_vocab.txt --workers 20 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[57], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfairseq_preprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgood\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mdestdir\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_paired_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfairseq_preprocess\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtrainpref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_paired_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mvalidpref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_paired_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdev\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                      \u001B[49m\u001B[43msrcdict\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtoken_vocab.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[56], line 21\u001B[0m, in \u001B[0;36mfairseq_preprocess\u001B[1;34m(src, tgt, destdir, trainpref, validpref, testpref, srcdict, **kwargs)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(cmd)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# !{cmd}\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[43mrun_and_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[55], line 6\u001B[0m, in \u001B[0;36mrun_and_stream\u001B[1;34m(cmd)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_and_stream\u001B[39m(cmd):\n\u001B[0;32m      5\u001B[0m     proc \u001B[38;5;241m=\u001B[39m subprocess\u001B[38;5;241m.\u001B[39mPopen(cmd, stdout\u001B[38;5;241m=\u001B[39msubprocess\u001B[38;5;241m.\u001B[39mPIPE, stderr\u001B[38;5;241m=\u001B[39msubprocess\u001B[38;5;241m.\u001B[39mPIPE, text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28miter\u001B[39m(proc\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mreadline, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(line, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Print without newline for streaming effect\u001B[39;00m\n\u001B[0;32m      8\u001B[0m         sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()  \u001B[38;5;66;03m# Ensure immediate display\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\encodings\\cp1252.py:22\u001B[0m, in \u001B[0;36mIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mIncrementalDecoder\u001B[39;00m(codecs\u001B[38;5;241m.\u001B[39mIncrementalDecoder):\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m codecs\u001B[38;5;241m.\u001B[39mcharmap_decode(\u001B[38;5;28minput\u001B[39m,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,decoding_table)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9pY-SAZGVpP"
   },
   "source": [
    "## Train fixer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nlaNGVmD_V6",
    "outputId": "9a03a0a0-c907-4b9f-97a2-8451ddcb1265",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:54:09.477285Z",
     "start_time": "2024-07-11T15:54:09.456322Z"
    }
   },
   "source": [
    "# Train\n",
    "gpu_id = 0\n",
    "max_epoch = 2\n",
    "\n",
    "save_dir = round_dir/'model-fixer'\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "fairseq_train(gpu_id, str(fairseq_dir), str(save_dir), str(save_dir/'train.log.txt'),\n",
    "                    src='bad', tgt='good',\n",
    "                    criterion='label_smoothed_cross_entropy', label_smoothing=0.1,\n",
    "                    lr=1e-3, warmup_init_lr=1e-4, memory_efficient_fp16=True,\n",
    "                    encoder_layers=4, decoder_layers=4, encoder_embed_dim=256, decoder_embed_dim=256,\n",
    "                    encoder_ffn_embed_dim=1024, decoder_ffn_embed_dim=1024,\n",
    "                    max_tokens=13500, update_freq=2,\n",
    "                    max_epoch=max_epoch, save_interval_updates=10000, num_workers=4,\n",
    "                )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0  fairseq-train                 data\\round_0\\small_data_paired\\fairseq_preprocess                --source-lang bad --target-lang good                --arch transformer --share-all-embeddings                --encoder-layers 4 --decoder-layers 4                --encoder-embed-dim 256 --decoder-embed-dim 256                --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024                --encoder-attention-heads 8 --decoder-attention-heads 8                --encoder-normalize-before --decoder-normalize-before                --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2                --weight-decay 0.0001                --criterion label_smoothed_cross_entropy                --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1                --lr-scheduler inverse_sqrt --warmup-updates 400 --warmup-init-lr 0.0001                --lr 0.001                --max-tokens 13500                --update-freq 2                --max-epoch 2 --save-interval 1 --save-dir data\\round_0\\model-fixer --label-smoothing 0.1 --save-interval-updates 10000 --num-workers 4 --memory-efficient-fp16   2>&1 | tee -a data\\round_0\\model-fixer\\train.log.txt \n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "IBOlSanEMJa-"
   },
   "cell_type": "markdown",
   "source": [
    "## Preprocess data for fixer"
   ]
  },
  {
   "metadata": {
    "id": "jNAjFOsaMJa-",
    "outputId": "17d05a9b-abdb-4b10-c6db-566a8009ba5d",
    "ExecuteTime": {
     "end_time": "2024-07-14T01:22:07.668923Z",
     "start_time": "2024-07-14T01:21:17.235435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess input\n",
    "for split in range(n_splits):\n",
    "    destdir    = destdir_root/f'fairseq_preprocess__orig_bad.{split}'\n",
    "    if os.path.exists(str(destdir)):\n",
    "        continue\n",
    "    fairseq_preprocess(src='bad', tgt='good', workers=10,\n",
    "                       destdir  = str(destdir),\n",
    "                       testpref = str(data_dir/f'orig_bad_code/orig.{split}'),\n",
    "                       srcdict  = str(data_dir/'token_vocab.txt'),\n",
    "                       only_source=True )\n",
    "    shutil.copy(str(data_dir/'token_vocab.txt'), str(destdir/'dict.good.txt'))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "OALuxFlbMJa-"
   },
   "cell_type": "markdown",
   "source": [
    "## Run fixer"
   ]
  },
  {
   "metadata": {
    "id": "yP3A6T_uMJa-",
    "outputId": "f4d0089a-4879-4f8f-a939-f8075acce00d",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-18T08:37:55.067092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for split in range(n_splits):\n",
    "    destdir    = destdir_root/f'fairseq_preprocess__orig_bad.{split}'\n",
    "    pred_path  = destdir/'model-fixer.pred.txt'\n",
    "    fairseq_generate(str(destdir), str(model_path), str(pred_path),\n",
    "                         src='bad', tgt='good', gen_subset='test',\n",
    "                         beam=10, nbest=10, max_len_a=1, max_len_b=50, max_tokens=7000)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 15:37:56 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2024-07-18 15:37:57 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'data/round_0/model-fixer/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 7000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 7000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'nbest': 10, 'max_len_a': 1.0, 'max_len_b': 50, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data/round_0/orig_bad/fairseq_preprocess__orig_bad.0', 'source_lang': 'bad', 'target_lang': 'good', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2024-07-18 15:37:57 | INFO | fairseq.tasks.translation | [bad] dictionary: 10000 types\n",
      "2024-07-18 15:37:57 | INFO | fairseq.tasks.translation | [good] dictionary: 10000 types\n",
      "2024-07-18 15:37:57 | INFO | fairseq_cli.generate | loading model(s) from data/round_0/model-fixer/checkpoint_best.pt\n",
      "2024-07-18 15:37:57 | INFO | fairseq.data.data_utils | loaded 7,528 examples from: data/round_0/orig_bad/fairseq_preprocess__orig_bad.0/test.bad-good.bad\n",
      "2024-07-18 15:37:57 | INFO | fairseq.tasks.translation | data/round_0/orig_bad/fairseq_preprocess__orig_bad.0 test bad-good 7528 examples\n",
      "2024-07-18 15:37:58 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "x9HbGkFuMJa-"
   },
   "cell_type": "markdown",
   "source": [
    "## Evaluate fixer"
   ]
  },
  {
   "metadata": {
    "id": "gOfPzaKzMJa_",
    "outputId": "1dfa70be-7f3c-4387-bff7-9b3e84bb6077",
    "ExecuteTime": {
     "end_time": "2024-07-14T02:08:22.261457Z",
     "start_time": "2024-07-14T02:06:41.808817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all the original bad code is split into 5 chunks for faster processing\n",
    "for split in range(n_splits):\n",
    "    eval_one_split(pred_dir_prefix, split, pred_fname, n_workers=10)\n",
    "\n",
    "get_test_result(pred_dir_prefix, pred_fname)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preds) 7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7528it [00:16, 458.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preds) 7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7528it [00:15, 470.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preds) 7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7528it [00:15, 472.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preds) 7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7528it [00:15, 477.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preds) 7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7527it [00:16, 461.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 3\n",
      "split 4\n",
      "Total\n",
      "   acc: 8031 (53.3%) | denom 15055\n",
      "--------------------------------------------------\n",
      "Unbalanced (){}[]\n",
      "   acc: 2871 (71.8%) | denom 3999\n",
      "Invalid syntax\n",
      "   acc: 2316 (48.8%) | denom 4749\n",
      "Indentation error\n",
      "   acc: 2844 (45.1%) | denom 6307\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "id": "D8oifIGSMJa_"
   },
   "cell_type": "markdown",
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Round 1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwxK8uD3HQKe"
   },
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N0MH3xgJHPhG",
    "outputId": "d5261d93-3c34-4626-830b-af445d514fec",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:20:21.763797Z",
     "start_time": "2024-07-11T15:20:18.115180Z"
    }
   },
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "process = subprocess.Popen([\"fairseq-preprocess\", \"--help\"], stdout=subprocess.PIPE, universal_newlines=True)\n",
    "for line in process.stdout:\n",
    "  print(line, end='')  # Print without newline to avoid extra line breaks\n",
    "  sys.stdout.flush()  # Flush output buffer to display immediately\n",
    "\n",
    "# Wait for the process to finish (optional)\n",
    "process.wait()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fairseq-preprocess [-h] [--no-progress-bar]\n",
      "                          [--log-interval LOG_INTERVAL]\n",
      "                          [--log-format {json,none,simple,tqdm}]\n",
      "                          [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n",
      "                          [--aim-run-hash AIM_RUN_HASH]\n",
      "                          [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                          [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                          [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                          [--memory-efficient-bf16] [--fp16]\n",
      "                          [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
      "                          [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                          [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                          [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                          [--on-cpu-convert-precision]\n",
      "                          [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                          [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                          [--amp] [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                          [--amp-init-scale AMP_INIT_SCALE]\n",
      "                          [--amp-scale-window AMP_SCALE_WINDOW]\n",
      "                          [--user-dir USER_DIR]\n",
      "                          [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                          [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                          [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                          [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                          [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                          [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                          [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                          [--tokenizer {moses,nltk,space}]\n",
      "                          [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                          [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                          [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                          [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                          [--task TASK] [-s SRC] [-t TARGET] [--trainpref FP]\n",
      "                          [--validpref FP] [--testpref FP] [--align-suffix FP]\n",
      "                          [--destdir DIR] [--thresholdtgt N]\n",
      "                          [--thresholdsrc N] [--tgtdict FP] [--srcdict FP]\n",
      "                          [--nwordstgt N] [--nwordssrc N] [--alignfile ALIGN]\n",
      "                          [--dataset-impl FORMAT] [--joined-dictionary]\n",
      "                          [--only-source] [--padding-factor N] [--workers N]\n",
      "                          [--dict-only]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --no-progress-bar     disable progress bar\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        log progress every N batches (when progress bar is\n",
      "                        disabled)\n",
      "  --log-format {json,none,simple,tqdm}\n",
      "                        log format to use\n",
      "  --log-file LOG_FILE   log file to copy metrics to.\n",
      "  --aim-repo AIM_REPO   path to Aim repository\n",
      "  --aim-run-hash AIM_RUN_HASH\n",
      "                        Aim run hash. If skipped, creates or continues run\n",
      "                        based on save_dir\n",
      "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
      "                        path to save logs for tensorboard, should match\n",
      "                        --logdir of running tensorboard (default: no\n",
      "                        tensorboard logging)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Weights and Biases project name to use for logging\n",
      "  --azureml-logging     Log scalars to AzureML context\n",
      "  --seed SEED           pseudo random number generator seed\n",
      "  --cpu                 use CPU instead of CUDA\n",
      "  --tpu                 use TPU instead of CUDA\n",
      "  --bf16                use bfloat16; implies --tpu\n",
      "  --memory-efficient-bf16\n",
      "                        use a memory-efficient version of BF16 training;\n",
      "                        implies --bf16\n",
      "  --fp16                use FP16\n",
      "  --memory-efficient-fp16\n",
      "                        use a memory-efficient version of FP16 training;\n",
      "                        implies --fp16\n",
      "  --fp16-no-flatten-grads\n",
      "                        don't flatten FP16 grads tensor\n",
      "  --fp16-init-scale FP16_INIT_SCALE\n",
      "                        default FP16 loss scale\n",
      "  --fp16-scale-window FP16_SCALE_WINDOW\n",
      "                        number of updates before increasing loss scale\n",
      "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
      "                        pct of updates that can overflow before decreasing the\n",
      "                        loss scale\n",
      "  --on-cpu-convert-precision\n",
      "                        if set, the floating point conversion to fp16/bf16\n",
      "                        runs on CPU. This reduces bus transfer time and GPU\n",
      "                        memory usage.\n",
      "  --min-loss-scale MIN_LOSS_SCALE\n",
      "                        minimum FP16/AMP loss scale, after which training is\n",
      "                        stopped\n",
      "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
      "                        threshold FP16 loss scale from below\n",
      "  --amp                 use automatic mixed precision\n",
      "  --amp-batch-retries AMP_BATCH_RETRIES\n",
      "                        number of retries of same batch after reducing loss\n",
      "                        scale with AMP\n",
      "  --amp-init-scale AMP_INIT_SCALE\n",
      "                        default AMP loss scale\n",
      "  --amp-scale-window AMP_SCALE_WINDOW\n",
      "                        number of updates before increasing AMP loss scale\n",
      "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
      "                        (tasks and/or architectures)\n",
      "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
      "                        how often to clear the PyTorch CUDA cache (0 to\n",
      "                        disable)\n",
      "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
      "                        number of bytes reserved for gathering stats from\n",
      "                        workers\n",
      "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
      "                        total number of GPUs to parallelize model over\n",
      "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
      "                        path to quantization config file\n",
      "  --profile             enable autograd profiler emit_nvtx\n",
      "  --reset-logging       when using Hydra, reset the logging at the beginning\n",
      "                        of training\n",
      "  --suppress-crashes    suppress crashes when training with the hydra_train\n",
      "                        entry point so that the main method can return a value\n",
      "                        (useful for sweeps)\n",
      "  --use-plasma-view     Store indices and sizes in shared memory\n",
      "  --plasma-path PLASMA_PATH\n",
      "                        path to run plasma_store, defaults to /tmp/plasma.\n",
      "                        Paths outside /tmp tend to fail.\n",
      "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
      "  --tokenizer {moses,nltk,space}\n",
      "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
      "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
      "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
      "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
      "  --task TASK           task\n",
      "  --dataset-impl FORMAT\n",
      "                        output dataset implementation\n",
      "\n",
      "Preprocessing:\n",
      "  -s SRC, --source-lang SRC\n",
      "                        source language\n",
      "  -t TARGET, --target-lang TARGET\n",
      "                        target language\n",
      "  --trainpref FP        train file prefix (also used to build dictionaries)\n",
      "  --validpref FP        comma separated, valid file prefixes (words missing\n",
      "                        from train set are replaced with <unk>)\n",
      "  --testpref FP         comma separated, test file prefixes (words missing\n",
      "                        from train set are replaced with <unk>)\n",
      "  --align-suffix FP     alignment file suffix\n",
      "  --destdir DIR         destination dir\n",
      "  --thresholdtgt N      map words appearing less than threshold times to\n",
      "                        unknown\n",
      "  --thresholdsrc N      map words appearing less than threshold times to\n",
      "                        unknown\n",
      "  --tgtdict FP          reuse given target dictionary\n",
      "  --srcdict FP          reuse given source dictionary\n",
      "  --nwordstgt N         number of target words to retain\n",
      "  --nwordssrc N         number of source words to retain\n",
      "  --alignfile ALIGN     an alignment file (optional)\n",
      "  --joined-dictionary   Generate joined dictionary\n",
      "  --only-source         Only process the source language\n",
      "  --padding-factor N    Pad dictionary size to be multiple of N\n",
      "  --workers N           number of parallel workers\n",
      "  --dict-only           if true, only builds a dictionary and then exits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TnGpVGlJLt3u",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:26:13.044034Z",
     "start_time": "2024-07-11T15:26:13.027029Z"
    }
   },
   "source": [
    "def hello_exe():\n",
    "    cmd = 'ping 8.8.8.8'\n",
    "    !{cmd}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:26:16.747294Z",
     "start_time": "2024-07-11T15:26:13.642111Z"
    },
    "id": "DefgRiNuj4DC",
    "outputId": "87ce1cc4-aa9f-4b2a-8b24-c8151a0864e2"
   },
   "cell_type": "code",
   "source": [
    "hello_exe()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pinging 8.8.8.8 with 32 bytes of data:\n",
      "Reply from 8.8.8.8: bytes=32 time=43ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=39ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=36ms TTL=114\n",
      "\n",
      "Ping statistics for 8.8.8.8:\n",
      "    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),\n",
      "Approximate round trip times in milli-seconds:\n",
      "    Minimum = 36ms, Maximum = 43ms, Average = 39ms\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:28:00.126864Z",
     "start_time": "2024-07-11T15:28:00.056190Z"
    },
    "id": "xaA7YK8zj4DC",
    "outputId": "429410b1-cab4-4676-ecfc-39c5342d2a28"
   },
   "cell_type": "code",
   "source": [
    "from wurlitzer import sys_pipes\n",
    "\n",
    "with sys_pipes():\n",
    "    !ping 8.8.8.8\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fcntl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwurlitzer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipes\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pipes():\n\u001B[0;32m      4\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mping 8.8.8.8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\wurlitzer.py:32\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m contextmanager\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfcntl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m F_GETFL, F_SETFL, fcntl\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqueue\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Queue\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'fcntl'"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:30:03.109818Z",
     "start_time": "2024-07-11T15:30:02.994828Z"
    },
    "id": "IzepEpcWj4DC",
    "outputId": "7eb24ce3-cdf2-47af-980c-2b497f71b3d4"
   },
   "cell_type": "code",
   "source": [
    "%load_ext wurlitzer\n",
    "\n",
    "!echo 'Hello'"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fcntl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mload_ext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwurlitzer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mecho \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHello\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2478\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2480\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2484\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\magics\\extension.py:33\u001B[0m, in \u001B[0;36mExtensionMagics.load_ext\u001B[1;34m(self, module_str)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m module_str:\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UsageError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing module name.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malready loaded\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m extension is already loaded. To reload it, use:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m module_str)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\extensions.py:62\u001B[0m, in \u001B[0;36mExtensionManager.load_extension\u001B[1;34m(self, module_str)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \n\u001B[0;32m     57\u001B[0m \u001B[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124;03mfunction, or None if it succeeded.\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01min\u001B[39;00m BUILTINS_EXTS:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\extensions.py:77\u001B[0m, in \u001B[0;36mExtensionManager._load_extension\u001B[1;34m(self, module_str)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmodules:\n\u001B[1;32m---> 77\u001B[0m         mod \u001B[38;5;241m=\u001B[39m \u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     mod \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules[module_str]\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_load_ipython_extension(mod):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\wurlitzer.py:32\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m contextmanager\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfcntl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m F_GETFL, F_SETFL, fcntl\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqueue\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Queue\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'fcntl'"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:48:09.019745Z",
     "start_time": "2024-07-11T15:48:05.928576Z"
    },
    "id": "2g1jGZU4j4DD",
    "outputId": "3c73e70a-e062-4a81-fdcd-c52d8bb2e181"
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_and_stream(cmd):\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    for line in iter(proc.stdout.readline, ''):\n",
    "        print(line, end='')  # Print without newline for streaming effect\n",
    "        sys.stdout.flush()  # Ensure immediate display\n",
    "    proc.stdout.close()\n",
    "\n",
    "run_and_stream(\"ping 8.8.8.8\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pinging 8.8.8.8 with 32 bytes of data:\n",
      "Reply from 8.8.8.8: bytes=32 time=37ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "\n",
      "Ping statistics for 8.8.8.8:\n",
      "    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),\n",
      "Approximate round trip times in milli-seconds:\n",
      "    Minimum = 37ms, Maximum = 38ms, Average = 37ms\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:47:35.765607Z",
     "start_time": "2024-07-11T15:47:32.003606Z"
    },
    "id": "vBlKcpiEj4DD",
    "outputId": "71c27b39-3ad5-43b0-c2d7-fd0d1633b9f9"
   },
   "cell_type": "code",
   "source": [
    "run_and_stream(\"fairseq-train -h\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD ['fairseq-train', '-h']\n",
      "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                     [--log-format {json,none,simple,tqdm}]\n",
      "                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n",
      "                     [--aim-run-hash AIM_RUN_HASH]\n",
      "                     [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                     [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                     [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                     [--memory-efficient-bf16] [--fp16]\n",
      "                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
      "                     [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                     [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                     [--on-cpu-convert-precision]\n",
      "                     [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                     [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                     [--amp-init-scale AMP_INIT_SCALE]\n",
      "                     [--amp-scale-window AMP_SCALE_WINDOW]\n",
      "                     [--user-dir USER_DIR]\n",
      "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                     [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                     [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                     [--tokenizer {moses,nltk,space}]\n",
      "                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                     [--task TASK] [--num-workers NUM_WORKERS]\n",
      "                     [--skip-invalid-size-inputs-valid-test]\n",
      "                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                     [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                     [--train-subset TRAIN_SUBSET]\n",
      "                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]\n",
      "                     [--ignore-unused-valid-subsets]\n",
      "                     [--validate-interval VALIDATE_INTERVAL]\n",
      "                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                     [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                     [--disable-validation]\n",
      "                     [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                     [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                     [--max-valid-steps MAX_VALID_STEPS]\n",
      "                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\n",
      "                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n",
      "                     [--grouped-shuffling]\n",
      "                     [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                     [--update-ordered-indices-seed]\n",
      "                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                     [--distributed-rank DISTRIBUTED_RANK]\n",
      "                     [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                     [--distributed-port DISTRIBUTED_PORT]\n",
      "                     [--device-id DEVICE_ID] [--distributed-no-spawn]\n",
      "                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                     [--ddp-comm-hook {none,fp16}]\n",
      "                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\n",
      "                     [--find-unused-parameters] [--gradient-as-bucket-view]\n",
      "                     [--fast-stat-sync]\n",
      "                     [--heartbeat-timeout HEARTBEAT_TIMEOUT]\n",
      "                     [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                     [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                     [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                     [--pipeline-model-parallel]\n",
      "                     [--pipeline-balance PIPELINE_BALANCE]\n",
      "                     [--pipeline-devices PIPELINE_DEVICES]\n",
      "                     [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                     [--pipeline-checkpoint {always,never,except_last}]\n",
      "                     [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                     [--fp32-reduce-scatter] [--cpu-offload]\n",
      "                     [--use-sharded-state] [--not-fsdp-flatten-parameters]\n",
      "                     [--arch ARCH] [--max-epoch MAX_EPOCH]\n",
      "                     [--max-update MAX_UPDATE]\n",
      "                     [--stop-time-hours STOP_TIME_HOURS]\n",
      "                     [--clip-norm CLIP_NORM] [--sentence-avg]\n",
      "                     [--update-freq UPDATE_FREQ] [--lr LR]\n",
      "                     [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n",
      "                     [--skip-remainder-batch] [--save-dir SAVE_DIR]\n",
      "                     [--restore-file RESTORE_FILE]\n",
      "                     [--continue-once CONTINUE_ONCE]\n",
      "                     [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                     [--reset-dataloader] [--reset-lr-scheduler]\n",
      "                     [--reset-meters] [--reset-optimizer]\n",
      "                     [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                     [--save-interval SAVE_INTERVAL]\n",
      "                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                     [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\n",
      "                     [--no-save] [--no-epoch-checkpoints]\n",
      "                     [--no-last-checkpoints] [--no-save-optimizer-state]\n",
      "                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                     [--load-checkpoint-on-all-dp-ranks]\n",
      "                     [--write-checkpoints-asynchronously] [--store-ema]\n",
      "                     [--ema-decay EMA_DECAY]\n",
      "                     [--ema-start-update EMA_START_UPDATE]\n",
      "                     [--ema-seed-model EMA_SEED_MODEL]\n",
      "                     [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --no-progress-bar     disable progress bar\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        log progress every N batches (when progress bar is\n",
      "                        disabled)\n",
      "  --log-format {json,none,simple,tqdm}\n",
      "                        log format to use\n",
      "  --log-file LOG_FILE   log file to copy metrics to.\n",
      "  --aim-repo AIM_REPO   path to Aim repository\n",
      "  --aim-run-hash AIM_RUN_HASH\n",
      "                        Aim run hash. If skipped, creates or continues run\n",
      "                        based on save_dir\n",
      "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
      "                        path to save logs for tensorboard, should match\n",
      "                        --logdir of running tensorboard (default: no\n",
      "                        tensorboard logging)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Weights and Biases project name to use for logging\n",
      "  --azureml-logging     Log scalars to AzureML context\n",
      "  --seed SEED           pseudo random number generator seed\n",
      "  --cpu                 use CPU instead of CUDA\n",
      "  --tpu                 use TPU instead of CUDA\n",
      "  --bf16                use bfloat16; implies --tpu\n",
      "  --memory-efficient-bf16\n",
      "                        use a memory-efficient version of BF16 training;\n",
      "                        implies --bf16\n",
      "  --fp16                use FP16\n",
      "  --memory-efficient-fp16\n",
      "                        use a memory-efficient version of FP16 training;\n",
      "                        implies --fp16\n",
      "  --fp16-no-flatten-grads\n",
      "                        don't flatten FP16 grads tensor\n",
      "  --fp16-init-scale FP16_INIT_SCALE\n",
      "                        default FP16 loss scale\n",
      "  --fp16-scale-window FP16_SCALE_WINDOW\n",
      "                        number of updates before increasing loss scale\n",
      "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
      "                        pct of updates that can overflow before decreasing the\n",
      "                        loss scale\n",
      "  --on-cpu-convert-precision\n",
      "                        if set, the floating point conversion to fp16/bf16\n",
      "                        runs on CPU. This reduces bus transfer time and GPU\n",
      "                        memory usage.\n",
      "  --min-loss-scale MIN_LOSS_SCALE\n",
      "                        minimum FP16/AMP loss scale, after which training is\n",
      "                        stopped\n",
      "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
      "                        threshold FP16 loss scale from below\n",
      "  --amp                 use automatic mixed precision\n",
      "  --amp-batch-retries AMP_BATCH_RETRIES\n",
      "                        number of retries of same batch after reducing loss\n",
      "                        scale with AMP\n",
      "  --amp-init-scale AMP_INIT_SCALE\n",
      "                        default AMP loss scale\n",
      "  --amp-scale-window AMP_SCALE_WINDOW\n",
      "                        number of updates before increasing AMP loss scale\n",
      "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
      "                        (tasks and/or architectures)\n",
      "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
      "                        how often to clear the PyTorch CUDA cache (0 to\n",
      "                        disable)\n",
      "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
      "                        number of bytes reserved for gathering stats from\n",
      "                        workers\n",
      "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
      "                        total number of GPUs to parallelize model over\n",
      "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
      "                        path to quantization config file\n",
      "  --profile             enable autograd profiler emit_nvtx\n",
      "  --reset-logging       when using Hydra, reset the logging at the beginning\n",
      "                        of training\n",
      "  --suppress-crashes    suppress crashes when training with the hydra_train\n",
      "                        entry point so that the main method can return a value\n",
      "                        (useful for sweeps)\n",
      "  --use-plasma-view     Store indices and sizes in shared memory\n",
      "  --plasma-path PLASMA_PATH\n",
      "                        path to run plasma_store, defaults to /tmp/plasma.\n",
      "                        Paths outside /tmp tend to fail.\n",
      "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
      "  --tokenizer {moses,nltk,space}\n",
      "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
      "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
      "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
      "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
      "  --task TASK           task\n",
      "\n",
      "dataset_data_loading:\n",
      "  --num-workers NUM_WORKERS\n",
      "                        how many subprocesses to use for data loading\n",
      "  --skip-invalid-size-inputs-valid-test\n",
      "                        ignore too long or too short lines in valid and test\n",
      "                        set\n",
      "  --max-tokens MAX_TOKENS\n",
      "                        maximum number of tokens in a batch\n",
      "  --batch-size BATCH_SIZE, --max-sentences BATCH_SIZE\n",
      "                        number of examples in a batch\n",
      "  --required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE\n",
      "                        batch size will be a multiplier of this value\n",
      "  --required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE\n",
      "                        maximum sequence length in batch will be a multiplier\n",
      "                        of this value\n",
      "  --dataset-impl {raw,lazy,cached,mmap,fasta,huffman}\n",
      "                        output dataset implementation\n",
      "  --data-buffer-size DATA_BUFFER_SIZE\n",
      "                        Number of batches to preload\n",
      "  --train-subset TRAIN_SUBSET\n",
      "                        data subset to use for training (e.g. train, valid,\n",
      "                        test)\n",
      "  --valid-subset VALID_SUBSET\n",
      "                        comma separated list of data subsets to use for\n",
      "                        validation (e.g. train, valid, test)\n",
      "  --combine-valid-subsets, --combine-val\n",
      "                        comma separated list of data subsets to use for\n",
      "                        validation (e.g. train, valid, test)\n",
      "  --ignore-unused-valid-subsets\n",
      "                        do not raise error if valid subsets are ignored\n",
      "  --validate-interval VALIDATE_INTERVAL\n",
      "                        validate every N epochs\n",
      "  --validate-interval-updates VALIDATE_INTERVAL_UPDATES\n",
      "                        validate every N updates\n",
      "  --validate-after-updates VALIDATE_AFTER_UPDATES\n",
      "                        dont validate until reaching this many updates\n",
      "  --fixed-validation-seed FIXED_VALIDATION_SEED\n",
      "                        specified random seed for validation\n",
      "  --disable-validation  disable validation\n",
      "  --max-tokens-valid MAX_TOKENS_VALID\n",
      "                        maximum number of tokens in a validation batch\n",
      "                        (defaults to --max-tokens)\n",
      "  --batch-size-valid BATCH_SIZE_VALID, --max-sentences-valid BATCH_SIZE_VALID\n",
      "                        batch size of the validation batch (defaults to\n",
      "                        --batch-size)\n",
      "  --max-valid-steps MAX_VALID_STEPS, --nval MAX_VALID_STEPS\n",
      "                        How many batches to evaluate\n",
      "  --curriculum CURRICULUM\n",
      "                        don't shuffle batches for first N epochs\n",
      "  --gen-subset GEN_SUBSET\n",
      "                        data subset to generate (train, valid, test)\n",
      "  --num-shards NUM_SHARDS\n",
      "                        shard generation over N shards\n",
      "  --shard-id SHARD_ID   id of the shard to generate (id < num_shards)\n",
      "  --grouped-shuffling   shuffle batches in groups of num_shards to enable\n",
      "                        similar sequence lengths on each GPU worker when\n",
      "                        batches are sorted by length\n",
      "  --update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR\n",
      "                        if true then prevents the reuse the epoch batch\n",
      "                        iterator by setting can_reuse_epoch_itr to false,\n",
      "                        defaults to --grouped-shuffling )\n",
      "  --update-ordered-indices-seed\n",
      "                        if true then increment seed with epoch for getting\n",
      "                        batch iterators, defautls to False.\n",
      "\n",
      "distributed_training:\n",
      "  --distributed-world-size DISTRIBUTED_WORLD_SIZE\n",
      "                        total number of GPUs across all nodes (default: all\n",
      "                        visible GPUs)\n",
      "  --distributed-num-procs DISTRIBUTED_NUM_PROCS\n",
      "                        total number of processes to fork (default: all\n",
      "                        visible GPUs)\n",
      "  --distributed-rank DISTRIBUTED_RANK\n",
      "                        rank of the current worker\n",
      "  --distributed-backend DISTRIBUTED_BACKEND\n",
      "                        distributed backend\n",
      "  --distributed-init-method DISTRIBUTED_INIT_METHOD\n",
      "                        typically tcp://hostname:port that will be used to\n",
      "                        establish initial connetion\n",
      "  --distributed-port DISTRIBUTED_PORT\n",
      "                        port number (not required if using --distributed-init-\n",
      "                        method)\n",
      "  --device-id DEVICE_ID, --local_rank DEVICE_ID\n",
      "                        which GPU to use (by default looks for $LOCAL_RANK,\n",
      "                        usually configured automatically)\n",
      "  --distributed-no-spawn\n",
      "                        do not spawn multiple processes even if multiple GPUs\n",
      "                        are visible\n",
      "  --ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}\n",
      "                        DistributedDataParallel backend\n",
      "  --ddp-comm-hook {none,fp16}\n",
      "                        communication hook\n",
      "  --bucket-cap-mb BUCKET_CAP_MB\n",
      "                        bucket size for reduction\n",
      "  --fix-batches-to-gpus\n",
      "                        don't shuffle batches between GPUs; this reduces\n",
      "                        overall randomness and may affect precision but avoids\n",
      "                        the cost of re-reading the data\n",
      "  --find-unused-parameters\n",
      "                        disable unused parameter detection (not applicable to\n",
      "                        --ddp-backend=legacy_ddp)\n",
      "  --gradient-as-bucket-view\n",
      "                        when set to True, gradients will be views pointing to\n",
      "                        different offsets of allreduce communication buckets.\n",
      "                        This can reduce peak memory usage, where the saved\n",
      "                        memory size will be equal to the total gradients size.\n",
      "                        --gradient-as-bucket-view=gradient_as_bucket_view)\n",
      "  --fast-stat-sync      [deprecated] this is now defined per Criterion\n",
      "  --heartbeat-timeout HEARTBEAT_TIMEOUT\n",
      "                        kill the job if no progress is made in N seconds; set\n",
      "                        to -1 to disable\n",
      "  --broadcast-buffers   Copy non-trainable parameters between GPUs, such as\n",
      "                        batchnorm population statistics\n",
      "  --slowmo-momentum SLOWMO_MOMENTUM\n",
      "                        SlowMo momentum term; by default use 0.0 for 16 GPUs,\n",
      "                        0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs\n",
      "  --slowmo-base-algorithm SLOWMO_BASE_ALGORITHM\n",
      "                        Base algorithm. Either 'localsgd' or 'sgp'. Please\n",
      "                        refer to the documentation of 'slowmo_base_algorithm'\n",
      "                        parameter in https://fairscale.readthedocs.io/en/lates\n",
      "                        t/api/experimental/nn/slowmo_ddp.html for more details\n",
      "  --localsgd-frequency LOCALSGD_FREQUENCY\n",
      "                        Local SGD allreduce frequency\n",
      "  --nprocs-per-node NPROCS_PER_NODE\n",
      "                        number of GPUs in each node. An allreduce operation\n",
      "                        across GPUs in a node is very fast. Hence, we do\n",
      "                        allreduce across GPUs in a node, and gossip across\n",
      "                        different nodes\n",
      "  --pipeline-model-parallel\n",
      "                        if set, use pipeline model parallelism across GPUs\n",
      "  --pipeline-balance PIPELINE_BALANCE\n",
      "                        partition the model into N_K pieces, where each piece\n",
      "                        contains N_i layers. The sum(args.pipeline_balance)\n",
      "                        should equal the total number of layers in the model\n",
      "  --pipeline-devices PIPELINE_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-balance\n",
      "                        argument\n",
      "  --pipeline-chunks PIPELINE_CHUNKS\n",
      "                        microbatch count for pipeline model parallelism\n",
      "  --pipeline-encoder-balance PIPELINE_ENCODER_BALANCE\n",
      "                        partition the pipeline parallel encoder into N_K\n",
      "                        pieces, where each piece contains N_i layers. The\n",
      "                        sum(args.pipeline_encoder_balance) should equal the\n",
      "                        total number of encoder layers in the model\n",
      "  --pipeline-encoder-devices PIPELINE_ENCODER_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-\n",
      "                        encoder-balance argument\n",
      "  --pipeline-decoder-balance PIPELINE_DECODER_BALANCE\n",
      "                        partition the pipeline parallel decoder into N_K\n",
      "                        pieces, where each piece contains N_i layers. The\n",
      "                        sum(args.pipeline_decoder_balance) should equal the\n",
      "                        total number of decoder layers in the model\n",
      "  --pipeline-decoder-devices PIPELINE_DECODER_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-\n",
      "                        decoder-balance argument\n",
      "  --pipeline-checkpoint {always,never,except_last}\n",
      "                        checkpointing mode for pipeline model parallelism\n",
      "  --zero-sharding {none,os}\n",
      "                        ZeRO sharding\n",
      "  --no-reshard-after-forward\n",
      "                        don't reshard parameters after forward pass\n",
      "  --fp32-reduce-scatter\n",
      "                        reduce-scatter grads in FP32\n",
      "  --cpu-offload         offload FP32 params to CPU\n",
      "  --use-sharded-state   use sharded checkpoint files\n",
      "  --not-fsdp-flatten-parameters\n",
      "                        not flatten parameter param for fsdp\n",
      "\n",
      "Model configuration:\n",
      "  --arch ARCH, -a ARCH  model architecture\n",
      "\n",
      "optimization:\n",
      "  --max-epoch MAX_EPOCH\n",
      "                        force stop training at specified epoch\n",
      "  --max-update MAX_UPDATE\n",
      "                        force stop training at specified update\n",
      "  --stop-time-hours STOP_TIME_HOURS\n",
      "                        force stop training after specified cumulative time\n",
      "                        (if >0)\n",
      "  --clip-norm CLIP_NORM\n",
      "                        clip threshold of gradients\n",
      "  --sentence-avg        normalize gradients by the number of sentences in a\n",
      "                        batch (default is to normalize by number of tokens)\n",
      "  --update-freq UPDATE_FREQ\n",
      "                        update parameters every N_i batches, when in epoch i\n",
      "  --lr LR               learning rate for the first N epochs; all epochs >N\n",
      "                        using LR_N (note: this may be interpreted differently\n",
      "                        depending on --lr-scheduler)\n",
      "  --stop-min-lr STOP_MIN_LR\n",
      "                        stop training when the learning rate reaches this\n",
      "                        minimum\n",
      "  --use-bmuf            specify global optimizer for syncing models on\n",
      "                        different GPUs/shards\n",
      "  --skip-remainder-batch\n",
      "                        if set, include the last (partial) batch of each epoch\n",
      "                        in training (default is to skip it).\n",
      "\n",
      "checkpoint:\n",
      "  --save-dir SAVE_DIR   path to save checkpoints\n",
      "  --restore-file RESTORE_FILE\n",
      "                        filename from which to load checkpoint (default:\n",
      "                        <save-dir>/checkpoint_last.pt\n",
      "  --continue-once CONTINUE_ONCE\n",
      "                        continues from this checkpoint, unless a checkpoint\n",
      "                        indicated in 'restore_file' option is present\n",
      "  --finetune-from-model FINETUNE_FROM_MODEL\n",
      "                        finetune from a pretrained model; note that meters and\n",
      "                        lr scheduler will be reset\n",
      "  --reset-dataloader    if set, does not reload dataloader state from the\n",
      "                        checkpoint\n",
      "  --reset-lr-scheduler  if set, does not load lr scheduler state from the\n",
      "                        checkpoint\n",
      "  --reset-meters        if set, does not load meters from the checkpoint\n",
      "  --reset-optimizer     if set, does not load optimizer state from the\n",
      "                        checkpoint\n",
      "  --optimizer-overrides OPTIMIZER_OVERRIDES\n",
      "                        a dictionary used to override optimizer args when\n",
      "                        loading a checkpoint\n",
      "  --save-interval SAVE_INTERVAL\n",
      "                        save a checkpoint every N epochs\n",
      "  --save-interval-updates SAVE_INTERVAL_UPDATES\n",
      "                        save a checkpoint (and validate) every N updates\n",
      "  --keep-interval-updates KEEP_INTERVAL_UPDATES\n",
      "                        keep the last N checkpoints saved with --save-\n",
      "                        interval-updates\n",
      "  --keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN\n",
      "                        when used with --keep-interval-updates, skips deleting\n",
      "                        any checkpoints with update X where X %\n",
      "                        keep_interval_updates_pattern == 0\n",
      "  --keep-last-epochs KEEP_LAST_EPOCHS\n",
      "                        keep last N epoch checkpoints\n",
      "  --keep-best-checkpoints KEEP_BEST_CHECKPOINTS\n",
      "                        keep best N checkpoints based on scores\n",
      "  --no-save             don't save models or checkpoints\n",
      "  --no-epoch-checkpoints\n",
      "                        only store last and best checkpoints\n",
      "  --no-last-checkpoints\n",
      "                        don't store last checkpoints\n",
      "  --no-save-optimizer-state\n",
      "                        don't save optimizer-state as part of checkpoint\n",
      "  --best-checkpoint-metric BEST_CHECKPOINT_METRIC\n",
      "                        metric to use for saving \"best\" checkpoints\n",
      "  --maximize-best-checkpoint-metric\n",
      "                        select the largest metric value for saving \"best\"\n",
      "                        checkpoints\n",
      "  --patience PATIENCE   early stop training if valid performance doesn't\n",
      "                        improve for N consecutive validation runs; note that\n",
      "                        this is influenced by --validate-interval\n",
      "  --checkpoint-suffix CHECKPOINT_SUFFIX\n",
      "                        suffix to add to the checkpoint file name\n",
      "  --checkpoint-shard-count CHECKPOINT_SHARD_COUNT\n",
      "                        Number of shards containing the checkpoint - if the\n",
      "                        checkpoint is over 300GB, it is preferable to split it\n",
      "                        into shards to prevent OOM on CPU while loading the\n",
      "                        checkpoint\n",
      "  --load-checkpoint-on-all-dp-ranks\n",
      "                        load checkpoints on all data parallel devices\n",
      "                        (default: only load on rank 0 and broadcast to other\n",
      "                        devices)\n",
      "  --write-checkpoints-asynchronously, --save-async\n",
      "                        Write checkpoints asynchronously in a separate thread.\n",
      "                        NOTE: This feature is currently being tested.\n",
      "\n",
      "EMA configuration:\n",
      "  --store-ema\n",
      "  --ema-decay EMA_DECAY\n",
      "                        decay for exponential moving average model\n",
      "  --ema-start-update EMA_START_UPDATE\n",
      "                        start EMA update after this many model updates\n",
      "  --ema-seed-model EMA_SEED_MODEL\n",
      "                        Seed to load EMA model from. Used to load EMA model\n",
      "                        separately from the actual model.\n",
      "  --ema-update-freq EMA_UPDATE_FREQ\n",
      "                        Do EMA update every this many model updates\n",
      "  --ema-fp32            If true, store EMA model in fp32 even if model is in\n",
      "                        fp16\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:46:31.631889Z",
     "start_time": "2024-07-11T15:46:31.332644Z"
    },
    "id": "FcAEH46qj4DD",
    "outputId": "771f7546-f55f-4351-b810-ffc08c04186c"
   },
   "cell_type": "code",
   "source": [
    "%run ping 8.8.8.8"
   ],
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'ping'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\magics\\execution.py:716\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[1;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[0;32m    715\u001B[0m     fpath \u001B[38;5;241m=\u001B[39m arg_lst[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m--> 716\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[43mfile_finder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\utils\\path.py:91\u001B[0m, in \u001B[0;36mget_py_filename\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m     90\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m py_name\n\u001B[1;32m---> 91\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile `\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m` not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m name)\n",
      "\u001B[1;31mOSError\u001B[0m: File `'ping'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mping 8.8.8.8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2478\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2480\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2484\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\magics\\execution.py:727\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[1;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.*\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m\"\u001B[39m,fpath):\n\u001B[0;32m    726\u001B[0m         warn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFor Windows, use double quotes to wrap a filename: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124mun \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmypath\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmyfile.py\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 727\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    729\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fpath \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmeta_path:\n",
      "\u001B[1;31mException\u001B[0m: File `'ping'` not found."
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "hs9cVzKpj4DD"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "\n",
    "\n",
    "@register_line_magic\n",
    "def runrealcmd(command):\n",
    "    process = Popen(command, stdout=PIPE, shell=True, stderr=STDOUT, bufsize=1, close_fds=True)\n",
    "    for line in iter(process.stdout.readline, b''):\n",
    "        print(line.rstrip().decode('utf-8'))\n",
    "    process.stdout.close()\n",
    "    process.wait()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
