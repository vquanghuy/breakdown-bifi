{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMh9r11F0BD9"
   },
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxK14PmC9Yzz"
   },
   "source": [
    "We should check if we're on the Colab and do additional setup\n",
    "- Install `fairseq`, `tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ngsjzyFax4KL",
    "outputId": "5f291fd0-b811-4ed0-acbd-02686a48fff5",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:04:33.236212Z",
     "start_time": "2024-07-11T15:04:33.228214Z"
    }
   },
   "source": [
    "from IPython.core import getipython\n",
    "\n",
    "is_colab = 'google.colab' in str(getipython.get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "  !pip install fairseq tqdm\n",
    "  print(\"Fairseq installation successful (if no errors occurred)\")\n",
    "else:\n",
    "  print(\"Notebook is not on Colab. Fairseq installation not attempted.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook is not on Colab. Fairseq installation not attempted.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rAuWy859T45",
    "outputId": "89cc16f7-d463-4718-f8af-660b24b95908",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:03:08.093854Z",
     "start_time": "2024-07-11T15:03:08.082766Z"
    }
   },
   "source": [
    "# Check PyTorch version\n",
    "import torch\n",
    "print('Torch', torch.__version__)\n",
    "\n",
    "import fairseq\n",
    "print('fairseq', fairseq.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 2.3.1\n",
      "fairseq 0.12.2\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNBdpnZp_bQt"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0q3s-dVP_L00",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:04:38.264472Z",
     "start_time": "2024-07-11T15:04:38.247956Z"
    }
   },
   "source": "DATA_DIR = 'drive/MyDrive/Dataset/bifi-dataset' if is_colab else 'data'",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4jFzSKSRYLk"
   },
   "source": [
    "## Data reduce\n",
    "\n",
    "Since the original dataset is huge, training faced lots of difficulty, so we'll reduce the size, just take a small subset for training / validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38ITJmmMBrcQ"
   },
   "source": [
    "# Supported functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "acqFRA0EHxRg"
   },
   "source": [
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "def run_and_stream(cmd):\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    for line in iter(proc.stdout.readline, ''):\n",
    "        print(line, end='')  # Print without newline for streaming effect\n",
    "        sys.stdout.flush()  # Ensure immediate display\n",
    "    proc.stdout.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qUgoaHVGnQy"
   },
   "source": [
    "## fairseq_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KHeGjQtqBtul",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:48:27.945152Z",
     "start_time": "2024-07-11T15:48:27.930232Z"
    }
   },
   "source": [
    "def fairseq_preprocess(src, tgt, destdir, trainpref=None, validpref=None, testpref=None, srcdict=None, **kwargs):\n",
    "    additional_cmds = ''.join([f\"--{k.replace('_', '-')} {v} \" for k, v in kwargs.items() if not isinstance(v, bool)])\n",
    "    additional_cmds += ''.join([f\"--{k.replace('_', '-')} \" for k, v in kwargs.items() if isinstance(v, bool) and v])\n",
    "    cmd = f'fairseq-preprocess --source-lang {src} --destdir {destdir} \\\n",
    "            --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 '\n",
    "    if tgt is not None:\n",
    "        cmd += f'--target-lang {tgt} '\n",
    "    if trainpref is not None:\n",
    "        cmd += f'--trainpref {trainpref} '\n",
    "    if validpref is not None:\n",
    "        cmd += f'--validpref {validpref} '\n",
    "    if testpref is not None:\n",
    "        cmd += f'--testpref {testpref} '\n",
    "    if srcdict is not None:\n",
    "        cmd += f'--srcdict {srcdict} '\n",
    "    cmd += additional_cmds\n",
    "\n",
    "    # Execute command line command\n",
    "    print(cmd)\n",
    "    !{cmd}"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALPMFOrMGok3"
   },
   "source": [
    "## fairseq_train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AGDCnx3zBukX",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:54:03.662852Z",
     "start_time": "2024-07-11T15:54:03.640775Z"
    }
   },
   "source": [
    "def fairseq_train(GPUs, preprocess_dir, save_dir, logfile, src, tgt, model='transformer',\n",
    "                  criterion='label_smoothed_cross_entropy',\n",
    "                  encoder_layers=4, decoder_layers=4, encoder_embed_dim=256,\n",
    "                  decoder_embed_dim=256, encoder_ffn_embed_dim=1024,\n",
    "                  decoder_ffn_embed_dim=1024, encoder_attention_heads=8,\n",
    "                  decoder_attention_heads=8, dropout=0.4,\n",
    "                  attention_dropout=0.2, relu_dropout=0.2,\n",
    "                  weight_decay=0.0001, warmup_updates=400, warmup_init_lr=1e-4,\n",
    "                  lr=1e-3, max_tokens=1000, update_freq=4,\n",
    "                  max_epoch=10, save_interval=1, log_interval=100, log_format='tqdm',\n",
    "                  user_dir=None, reset=False, restore_file=None, **kwargs):\n",
    "    if True:\n",
    "        additional_cmds = ''.join(\n",
    "            [f\"--{k.replace('_', '-')} {v} \" for k, v in kwargs.items() if not isinstance(v, bool)])\n",
    "        additional_cmds += ''.join(\n",
    "            [f\"--{k.replace('_', '-')} \" for k, v in kwargs.items() if isinstance(v, bool) and v])\n",
    "        cmd = f\"fairseq-train \\\n",
    "                {preprocess_dir} \\\n",
    "               --source-lang {src} --target-lang {tgt} \\\n",
    "               --arch {model} --share-all-embeddings \\\n",
    "               --encoder-layers {encoder_layers} --decoder-layers {decoder_layers} \\\n",
    "               --encoder-embed-dim {encoder_embed_dim} --decoder-embed-dim {decoder_embed_dim} \\\n",
    "               --encoder-ffn-embed-dim {encoder_ffn_embed_dim} --decoder-ffn-embed-dim {decoder_ffn_embed_dim} \\\n",
    "               --encoder-attention-heads {encoder_attention_heads} --decoder-attention-heads {decoder_attention_heads} \\\n",
    "               --encoder-normalize-before --decoder-normalize-before \\\n",
    "               --dropout {dropout} --attention-dropout {attention_dropout} --relu-dropout {relu_dropout} \\\n",
    "               --weight-decay {weight_decay} \\\n",
    "               --criterion {criterion} \\\n",
    "               --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1 \\\n",
    "               --lr-scheduler inverse_sqrt --warmup-updates {warmup_updates} --warmup-init-lr {warmup_init_lr} \\\n",
    "               --lr {lr} \\\n",
    "               --max-tokens {max_tokens} \\\n",
    "               --update-freq {update_freq} \\\n",
    "               --max-epoch {max_epoch} --save-interval {save_interval} --save-dir {save_dir} \"\n",
    "        if user_dir is not None:\n",
    "            cmd += f'--user-dir {user_dir} '\n",
    "        if restore_file is not None:\n",
    "            cmd += f\"--restore-file {restore_file} \"\n",
    "        if reset:\n",
    "            cmd += \"--reset-optimizer \\\n",
    "                   --reset-lr-scheduler \\\n",
    "                   --reset-dataloader \\\n",
    "                   --reset-meters \"\n",
    "        cmd += additional_cmds\n",
    "        if logfile is not None:\n",
    "            import socket, os\n",
    "            with open(logfile, 'w') as outf:\n",
    "                print(socket.gethostname(), file=outf)\n",
    "                print(\"pid:\", os.getpid(), file=outf)\n",
    "                print(\"screen: %s\" % subprocess.check_output('echo $STY', shell=True).decode('utf'), file=outf)\n",
    "                outf.flush()\n",
    "            cmd += f\"  2>&1 | tee -a {logfile} \"\n",
    "        if GPUs is not None:\n",
    "            cmd = 'CUDA_VISIBLE_DEVICES={}  {}'.format(GPUs, cmd)\n",
    "\n",
    "        print(cmd)\n",
    "        # !{cmd}"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPrCdDrwB14d"
   },
   "source": [
    "# Round 0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SWzdVzcktvpx",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:40:54.574001Z",
     "start_time": "2024-07-11T15:40:54.556837Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(DATA_DIR)\n",
    "round_dir = data_dir/'round_0'\n",
    "data_paired_dir = round_dir/'small_data_paired'\n",
    "fairseq_dir = data_paired_dir/'fairseq_preprocess'"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1MqIkQZsXzq2",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:40:10.886879Z",
     "start_time": "2024-07-11T15:40:10.659654Z"
    }
   },
   "source": [
    "# Remove fairseq preprocessed dir\n",
    "shutil.rmtree(str(fairseq_dir))"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data\\\\round_0\\\\small_data_paired\\\\fairseq_preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Remove fairseq preprocessed dir\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrmtree\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfairseq_dir\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\shutil.py:750\u001B[0m, in \u001B[0;36mrmtree\u001B[1;34m(path, ignore_errors, onerror)\u001B[0m\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;66;03m# can't continue even if onerror hook returns\u001B[39;00m\n\u001B[0;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 750\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\shutil.py:601\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    599\u001B[0m         entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(scandir_it)\n\u001B[0;32m    600\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[1;32m--> 601\u001B[0m     \u001B[43monerror\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscandir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    602\u001B[0m     entries \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    603\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m entries:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\shutil.py:598\u001B[0m, in \u001B[0;36m_rmtree_unsafe\u001B[1;34m(path, onerror)\u001B[0m\n\u001B[0;32m    596\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_rmtree_unsafe\u001B[39m(path, onerror):\n\u001B[0;32m    597\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 598\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscandir\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m scandir_it:\n\u001B[0;32m    599\u001B[0m             entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(scandir_it)\n\u001B[0;32m    600\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] The system cannot find the path specified: 'data\\\\round_0\\\\small_data_paired\\\\fairseq_preprocess'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U1F1hxikSXG7",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:11:26.075341Z",
     "start_time": "2024-07-11T15:11:23.003372Z"
    }
   },
   "source": [
    "# Take 1m lines as sample\n",
    "from itertools import islice\n",
    "\n",
    "original_data_paired_dir = round_dir/'data_paired'\n",
    "train_sliced_lines = 1000000\n",
    "dev_sliced_lines = train_sliced_lines / 100\n",
    "\n",
    "data_paired_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare train.good and train.bad\n",
    "with open(str(original_data_paired_dir/'train.good'), 'r', encoding='utf-8') as infile, \\\n",
    "    open(str(data_paired_dir/'train.good'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n",
    "\n",
    "with open(str(original_data_paired_dir/'train.bad'), 'r', encoding=\"utf-8\") as infile, \\\n",
    "    open(str(data_paired_dir/'train.bad'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n",
    "\n",
    "# Prepare dev.good and dev.bad\n",
    "with open(str(original_data_paired_dir/'dev.good'), 'r', encoding=\"utf-8\") as infile, \\\n",
    "    open(str(data_paired_dir/'dev.good'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n",
    "\n",
    "with open(str(original_data_paired_dir/'dev.bad'), 'r', encoding=\"utf-8\") as infile, \\\n",
    "    open(str(data_paired_dir/'dev.bad'), 'w', encoding='utf-8') as outfile:\n",
    "    for line in islice(infile, train_sliced_lines):\n",
    "        outfile.write(line)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkKlR8UWGSJR"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZs7SU74DRw6",
    "outputId": "a94d70a9-fa90-4194-aea5-9e02f3707442",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:52:02.567485Z",
     "start_time": "2024-07-11T15:48:34.270167Z"
    }
   },
   "source": [
    "fairseq_preprocess(src='bad', tgt='good', workers=20,\n",
    "                      destdir  = str(data_paired_dir/'fairseq_preprocess'),\n",
    "                      trainpref= str(data_paired_dir/'train'),\n",
    "                      validpref= str(data_paired_dir/'dev'),\n",
    "                      srcdict  = str(data_dir/'token_vocab.txt') )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-preprocess --source-lang bad --destdir data\\round_0\\small_data_paired\\fairseq_preprocess             --joined-dictionary --workers 50 --no-progress-bar --log-interval 20 --target-lang good --trainpref data\\round_0\\small_data_paired\\train --validpref data\\round_0\\small_data_paired\\dev --srcdict data\\token_vocab.txt --workers 20 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[57], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfairseq_preprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgood\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mdestdir\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_paired_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfairseq_preprocess\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtrainpref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_paired_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mvalidpref\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_paired_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdev\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                      \u001B[49m\u001B[43msrcdict\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtoken_vocab.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[56], line 21\u001B[0m, in \u001B[0;36mfairseq_preprocess\u001B[1;34m(src, tgt, destdir, trainpref, validpref, testpref, srcdict, **kwargs)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(cmd)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# !{cmd}\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[43mrun_and_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcmd\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[55], line 6\u001B[0m, in \u001B[0;36mrun_and_stream\u001B[1;34m(cmd)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_and_stream\u001B[39m(cmd):\n\u001B[0;32m      5\u001B[0m     proc \u001B[38;5;241m=\u001B[39m subprocess\u001B[38;5;241m.\u001B[39mPopen(cmd, stdout\u001B[38;5;241m=\u001B[39msubprocess\u001B[38;5;241m.\u001B[39mPIPE, stderr\u001B[38;5;241m=\u001B[39msubprocess\u001B[38;5;241m.\u001B[39mPIPE, text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28miter\u001B[39m(proc\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mreadline, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(line, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Print without newline for streaming effect\u001B[39;00m\n\u001B[0;32m      8\u001B[0m         sys\u001B[38;5;241m.\u001B[39mstdout\u001B[38;5;241m.\u001B[39mflush()  \u001B[38;5;66;03m# Ensure immediate display\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\encodings\\cp1252.py:22\u001B[0m, in \u001B[0;36mIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mIncrementalDecoder\u001B[39;00m(codecs\u001B[38;5;241m.\u001B[39mIncrementalDecoder):\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m codecs\u001B[38;5;241m.\u001B[39mcharmap_decode(\u001B[38;5;28minput\u001B[39m,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,decoding_table)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9pY-SAZGVpP"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nlaNGVmD_V6",
    "outputId": "9a03a0a0-c907-4b9f-97a2-8451ddcb1265",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:54:09.477285Z",
     "start_time": "2024-07-11T15:54:09.456322Z"
    }
   },
   "source": [
    "# Train\n",
    "# --gpu_id 0 --max_epoch 2\n",
    "gpu_id = 0\n",
    "max_epoch = 2\n",
    "\n",
    "save_dir = round_dir/'model-fixer'\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "fairseq_train(gpu_id, str(fairseq_dir), str(save_dir), str(save_dir/'train.log.txt'),\n",
    "                    src='bad', tgt='good',\n",
    "                    criterion='label_smoothed_cross_entropy', label_smoothing=0.1,\n",
    "                    lr=1e-3, warmup_init_lr=1e-4, memory_efficient_fp16=True,\n",
    "                    encoder_layers=4, decoder_layers=4, encoder_embed_dim=256, decoder_embed_dim=256,\n",
    "                    encoder_ffn_embed_dim=1024, decoder_ffn_embed_dim=1024,\n",
    "                    max_tokens=13500, update_freq=2,\n",
    "                    max_epoch=max_epoch, save_interval_updates=10000, num_workers=4,\n",
    "                )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES=0  fairseq-train                 data\\round_0\\small_data_paired\\fairseq_preprocess                --source-lang bad --target-lang good                --arch transformer --share-all-embeddings                --encoder-layers 4 --decoder-layers 4                --encoder-embed-dim 256 --decoder-embed-dim 256                --encoder-ffn-embed-dim 1024 --decoder-ffn-embed-dim 1024                --encoder-attention-heads 8 --decoder-attention-heads 8                --encoder-normalize-before --decoder-normalize-before                --dropout 0.4 --attention-dropout 0.2 --relu-dropout 0.2                --weight-decay 0.0001                --criterion label_smoothed_cross_entropy                --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 1                --lr-scheduler inverse_sqrt --warmup-updates 400 --warmup-init-lr 0.0001                --lr 0.001                --max-tokens 13500                --update-freq 2                --max-epoch 2 --save-interval 1 --save-dir data\\round_0\\model-fixer --label-smoothing 0.1 --save-interval-updates 10000 --num-workers 4 --memory-efficient-fp16   2>&1 | tee -a data\\round_0\\model-fixer\\train.log.txt \n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwxK8uD3HQKe"
   },
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "N0MH3xgJHPhG",
    "outputId": "d5261d93-3c34-4626-830b-af445d514fec",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:20:21.763797Z",
     "start_time": "2024-07-11T15:20:18.115180Z"
    }
   },
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "process = subprocess.Popen([\"fairseq-preprocess\", \"--help\"], stdout=subprocess.PIPE, universal_newlines=True)\n",
    "for line in process.stdout:\n",
    "  print(line, end='')  # Print without newline to avoid extra line breaks\n",
    "  sys.stdout.flush()  # Flush output buffer to display immediately\n",
    "\n",
    "# Wait for the process to finish (optional)\n",
    "process.wait()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: fairseq-preprocess [-h] [--no-progress-bar]\n",
      "                          [--log-interval LOG_INTERVAL]\n",
      "                          [--log-format {json,none,simple,tqdm}]\n",
      "                          [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n",
      "                          [--aim-run-hash AIM_RUN_HASH]\n",
      "                          [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                          [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                          [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                          [--memory-efficient-bf16] [--fp16]\n",
      "                          [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
      "                          [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                          [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                          [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                          [--on-cpu-convert-precision]\n",
      "                          [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                          [--threshold-loss-scale THRESHOLD_LOSS_SCALE]\n",
      "                          [--amp] [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                          [--amp-init-scale AMP_INIT_SCALE]\n",
      "                          [--amp-scale-window AMP_SCALE_WINDOW]\n",
      "                          [--user-dir USER_DIR]\n",
      "                          [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                          [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                          [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                          [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                          [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                          [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                          [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                          [--tokenizer {moses,nltk,space}]\n",
      "                          [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                          [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                          [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                          [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                          [--task TASK] [-s SRC] [-t TARGET] [--trainpref FP]\n",
      "                          [--validpref FP] [--testpref FP] [--align-suffix FP]\n",
      "                          [--destdir DIR] [--thresholdtgt N]\n",
      "                          [--thresholdsrc N] [--tgtdict FP] [--srcdict FP]\n",
      "                          [--nwordstgt N] [--nwordssrc N] [--alignfile ALIGN]\n",
      "                          [--dataset-impl FORMAT] [--joined-dictionary]\n",
      "                          [--only-source] [--padding-factor N] [--workers N]\n",
      "                          [--dict-only]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --no-progress-bar     disable progress bar\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        log progress every N batches (when progress bar is\n",
      "                        disabled)\n",
      "  --log-format {json,none,simple,tqdm}\n",
      "                        log format to use\n",
      "  --log-file LOG_FILE   log file to copy metrics to.\n",
      "  --aim-repo AIM_REPO   path to Aim repository\n",
      "  --aim-run-hash AIM_RUN_HASH\n",
      "                        Aim run hash. If skipped, creates or continues run\n",
      "                        based on save_dir\n",
      "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
      "                        path to save logs for tensorboard, should match\n",
      "                        --logdir of running tensorboard (default: no\n",
      "                        tensorboard logging)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Weights and Biases project name to use for logging\n",
      "  --azureml-logging     Log scalars to AzureML context\n",
      "  --seed SEED           pseudo random number generator seed\n",
      "  --cpu                 use CPU instead of CUDA\n",
      "  --tpu                 use TPU instead of CUDA\n",
      "  --bf16                use bfloat16; implies --tpu\n",
      "  --memory-efficient-bf16\n",
      "                        use a memory-efficient version of BF16 training;\n",
      "                        implies --bf16\n",
      "  --fp16                use FP16\n",
      "  --memory-efficient-fp16\n",
      "                        use a memory-efficient version of FP16 training;\n",
      "                        implies --fp16\n",
      "  --fp16-no-flatten-grads\n",
      "                        don't flatten FP16 grads tensor\n",
      "  --fp16-init-scale FP16_INIT_SCALE\n",
      "                        default FP16 loss scale\n",
      "  --fp16-scale-window FP16_SCALE_WINDOW\n",
      "                        number of updates before increasing loss scale\n",
      "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
      "                        pct of updates that can overflow before decreasing the\n",
      "                        loss scale\n",
      "  --on-cpu-convert-precision\n",
      "                        if set, the floating point conversion to fp16/bf16\n",
      "                        runs on CPU. This reduces bus transfer time and GPU\n",
      "                        memory usage.\n",
      "  --min-loss-scale MIN_LOSS_SCALE\n",
      "                        minimum FP16/AMP loss scale, after which training is\n",
      "                        stopped\n",
      "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
      "                        threshold FP16 loss scale from below\n",
      "  --amp                 use automatic mixed precision\n",
      "  --amp-batch-retries AMP_BATCH_RETRIES\n",
      "                        number of retries of same batch after reducing loss\n",
      "                        scale with AMP\n",
      "  --amp-init-scale AMP_INIT_SCALE\n",
      "                        default AMP loss scale\n",
      "  --amp-scale-window AMP_SCALE_WINDOW\n",
      "                        number of updates before increasing AMP loss scale\n",
      "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
      "                        (tasks and/or architectures)\n",
      "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
      "                        how often to clear the PyTorch CUDA cache (0 to\n",
      "                        disable)\n",
      "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
      "                        number of bytes reserved for gathering stats from\n",
      "                        workers\n",
      "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
      "                        total number of GPUs to parallelize model over\n",
      "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
      "                        path to quantization config file\n",
      "  --profile             enable autograd profiler emit_nvtx\n",
      "  --reset-logging       when using Hydra, reset the logging at the beginning\n",
      "                        of training\n",
      "  --suppress-crashes    suppress crashes when training with the hydra_train\n",
      "                        entry point so that the main method can return a value\n",
      "                        (useful for sweeps)\n",
      "  --use-plasma-view     Store indices and sizes in shared memory\n",
      "  --plasma-path PLASMA_PATH\n",
      "                        path to run plasma_store, defaults to /tmp/plasma.\n",
      "                        Paths outside /tmp tend to fail.\n",
      "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
      "  --tokenizer {moses,nltk,space}\n",
      "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
      "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
      "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
      "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
      "  --task TASK           task\n",
      "  --dataset-impl FORMAT\n",
      "                        output dataset implementation\n",
      "\n",
      "Preprocessing:\n",
      "  -s SRC, --source-lang SRC\n",
      "                        source language\n",
      "  -t TARGET, --target-lang TARGET\n",
      "                        target language\n",
      "  --trainpref FP        train file prefix (also used to build dictionaries)\n",
      "  --validpref FP        comma separated, valid file prefixes (words missing\n",
      "                        from train set are replaced with <unk>)\n",
      "  --testpref FP         comma separated, test file prefixes (words missing\n",
      "                        from train set are replaced with <unk>)\n",
      "  --align-suffix FP     alignment file suffix\n",
      "  --destdir DIR         destination dir\n",
      "  --thresholdtgt N      map words appearing less than threshold times to\n",
      "                        unknown\n",
      "  --thresholdsrc N      map words appearing less than threshold times to\n",
      "                        unknown\n",
      "  --tgtdict FP          reuse given target dictionary\n",
      "  --srcdict FP          reuse given source dictionary\n",
      "  --nwordstgt N         number of target words to retain\n",
      "  --nwordssrc N         number of source words to retain\n",
      "  --alignfile ALIGN     an alignment file (optional)\n",
      "  --joined-dictionary   Generate joined dictionary\n",
      "  --only-source         Only process the source language\n",
      "  --padding-factor N    Pad dictionary size to be multiple of N\n",
      "  --workers N           number of parallel workers\n",
      "  --dict-only           if true, only builds a dictionary and then exits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TnGpVGlJLt3u",
    "ExecuteTime": {
     "end_time": "2024-07-11T15:26:13.044034Z",
     "start_time": "2024-07-11T15:26:13.027029Z"
    }
   },
   "source": [
    "def hello_exe():\n",
    "    cmd = 'ping 8.8.8.8'\n",
    "    !{cmd}"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:26:16.747294Z",
     "start_time": "2024-07-11T15:26:13.642111Z"
    }
   },
   "cell_type": "code",
   "source": "hello_exe()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pinging 8.8.8.8 with 32 bytes of data:\n",
      "Reply from 8.8.8.8: bytes=32 time=43ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=39ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=36ms TTL=114\n",
      "\n",
      "Ping statistics for 8.8.8.8:\n",
      "    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),\n",
      "Approximate round trip times in milli-seconds:\n",
      "    Minimum = 36ms, Maximum = 43ms, Average = 39ms\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:28:00.126864Z",
     "start_time": "2024-07-11T15:28:00.056190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from wurlitzer import sys_pipes\n",
    "\n",
    "with sys_pipes():\n",
    "    !ping 8.8.8.8\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fcntl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwurlitzer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipes\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pipes():\n\u001B[0;32m      4\u001B[0m     get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mping 8.8.8.8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\wurlitzer.py:32\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m contextmanager\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfcntl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m F_GETFL, F_SETFL, fcntl\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqueue\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Queue\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'fcntl'"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:30:03.109818Z",
     "start_time": "2024-07-11T15:30:02.994828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext wurlitzer\n",
    "\n",
    "!echo 'Hello'"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fcntl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[33], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mload_ext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwurlitzer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39msystem(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mecho \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHello\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2478\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2480\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2484\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\magics\\extension.py:33\u001B[0m, in \u001B[0;36mExtensionMagics.load_ext\u001B[1;34m(self, module_str)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m module_str:\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m UsageError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing module name.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 33\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextension_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malready loaded\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m extension is already loaded. To reload it, use:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m module_str)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\extensions.py:62\u001B[0m, in \u001B[0;36mExtensionManager.load_extension\u001B[1;34m(self, module_str)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001B[39;00m\n\u001B[0;32m     56\u001B[0m \n\u001B[0;32m     57\u001B[0m \u001B[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;124;03mfunction, or None if it succeeded.\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_extension\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01min\u001B[39;00m BUILTINS_EXTS:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\extensions.py:77\u001B[0m, in \u001B[0;36mExtensionManager._load_extension\u001B[1;34m(self, module_str)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m     76\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module_str \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmodules:\n\u001B[1;32m---> 77\u001B[0m         mod \u001B[38;5;241m=\u001B[39m \u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     mod \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules[module_str]\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_load_ipython_extension(mod):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1050\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1027\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1006\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:688\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:883\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:241\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\wurlitzer.py:32\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m contextmanager\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfcntl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m F_GETFL, F_SETFL, fcntl\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lru_cache\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqueue\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Queue\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'fcntl'"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:48:09.019745Z",
     "start_time": "2024-07-11T15:48:05.928576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def run_and_stream(cmd):\n",
    "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    for line in iter(proc.stdout.readline, ''):\n",
    "        print(line, end='')  # Print without newline for streaming effect\n",
    "        sys.stdout.flush()  # Ensure immediate display\n",
    "    proc.stdout.close()\n",
    "\n",
    "run_and_stream(\"ping 8.8.8.8\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pinging 8.8.8.8 with 32 bytes of data:\n",
      "Reply from 8.8.8.8: bytes=32 time=37ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "Reply from 8.8.8.8: bytes=32 time=38ms TTL=114\n",
      "\n",
      "Ping statistics for 8.8.8.8:\n",
      "    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),\n",
      "Approximate round trip times in milli-seconds:\n",
      "    Minimum = 37ms, Maximum = 38ms, Average = 37ms\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:47:35.765607Z",
     "start_time": "2024-07-11T15:47:32.003606Z"
    }
   },
   "cell_type": "code",
   "source": "run_and_stream(\"fairseq-train -h\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD ['fairseq-train', '-h']\n",
      "usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]\n",
      "                     [--log-format {json,none,simple,tqdm}]\n",
      "                     [--log-file LOG_FILE] [--aim-repo AIM_REPO]\n",
      "                     [--aim-run-hash AIM_RUN_HASH]\n",
      "                     [--tensorboard-logdir TENSORBOARD_LOGDIR]\n",
      "                     [--wandb-project WANDB_PROJECT] [--azureml-logging]\n",
      "                     [--seed SEED] [--cpu] [--tpu] [--bf16]\n",
      "                     [--memory-efficient-bf16] [--fp16]\n",
      "                     [--memory-efficient-fp16] [--fp16-no-flatten-grads]\n",
      "                     [--fp16-init-scale FP16_INIT_SCALE]\n",
      "                     [--fp16-scale-window FP16_SCALE_WINDOW]\n",
      "                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]\n",
      "                     [--on-cpu-convert-precision]\n",
      "                     [--min-loss-scale MIN_LOSS_SCALE]\n",
      "                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE] [--amp]\n",
      "                     [--amp-batch-retries AMP_BATCH_RETRIES]\n",
      "                     [--amp-init-scale AMP_INIT_SCALE]\n",
      "                     [--amp-scale-window AMP_SCALE_WINDOW]\n",
      "                     [--user-dir USER_DIR]\n",
      "                     [--empty-cache-freq EMPTY_CACHE_FREQ]\n",
      "                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]\n",
      "                     [--model-parallel-size MODEL_PARALLEL_SIZE]\n",
      "                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]\n",
      "                     [--profile] [--reset-logging] [--suppress-crashes]\n",
      "                     [--use-plasma-view] [--plasma-path PLASMA_PATH]\n",
      "                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}]\n",
      "                     [--tokenizer {moses,nltk,space}]\n",
      "                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]\n",
      "                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}]\n",
      "                     [--lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}]\n",
      "                     [--scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}]\n",
      "                     [--task TASK] [--num-workers NUM_WORKERS]\n",
      "                     [--skip-invalid-size-inputs-valid-test]\n",
      "                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]\n",
      "                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]\n",
      "                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]\n",
      "                     [--dataset-impl {raw,lazy,cached,mmap,fasta,huffman}]\n",
      "                     [--data-buffer-size DATA_BUFFER_SIZE]\n",
      "                     [--train-subset TRAIN_SUBSET]\n",
      "                     [--valid-subset VALID_SUBSET] [--combine-valid-subsets]\n",
      "                     [--ignore-unused-valid-subsets]\n",
      "                     [--validate-interval VALIDATE_INTERVAL]\n",
      "                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]\n",
      "                     [--validate-after-updates VALIDATE_AFTER_UPDATES]\n",
      "                     [--fixed-validation-seed FIXED_VALIDATION_SEED]\n",
      "                     [--disable-validation]\n",
      "                     [--max-tokens-valid MAX_TOKENS_VALID]\n",
      "                     [--batch-size-valid BATCH_SIZE_VALID]\n",
      "                     [--max-valid-steps MAX_VALID_STEPS]\n",
      "                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]\n",
      "                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]\n",
      "                     [--grouped-shuffling]\n",
      "                     [--update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR]\n",
      "                     [--update-ordered-indices-seed]\n",
      "                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]\n",
      "                     [--distributed-num-procs DISTRIBUTED_NUM_PROCS]\n",
      "                     [--distributed-rank DISTRIBUTED_RANK]\n",
      "                     [--distributed-backend DISTRIBUTED_BACKEND]\n",
      "                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]\n",
      "                     [--distributed-port DISTRIBUTED_PORT]\n",
      "                     [--device-id DEVICE_ID] [--distributed-no-spawn]\n",
      "                     [--ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}]\n",
      "                     [--ddp-comm-hook {none,fp16}]\n",
      "                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]\n",
      "                     [--find-unused-parameters] [--gradient-as-bucket-view]\n",
      "                     [--fast-stat-sync]\n",
      "                     [--heartbeat-timeout HEARTBEAT_TIMEOUT]\n",
      "                     [--broadcast-buffers] [--slowmo-momentum SLOWMO_MOMENTUM]\n",
      "                     [--slowmo-base-algorithm SLOWMO_BASE_ALGORITHM]\n",
      "                     [--localsgd-frequency LOCALSGD_FREQUENCY]\n",
      "                     [--nprocs-per-node NPROCS_PER_NODE]\n",
      "                     [--pipeline-model-parallel]\n",
      "                     [--pipeline-balance PIPELINE_BALANCE]\n",
      "                     [--pipeline-devices PIPELINE_DEVICES]\n",
      "                     [--pipeline-chunks PIPELINE_CHUNKS]\n",
      "                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]\n",
      "                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]\n",
      "                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]\n",
      "                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]\n",
      "                     [--pipeline-checkpoint {always,never,except_last}]\n",
      "                     [--zero-sharding {none,os}] [--no-reshard-after-forward]\n",
      "                     [--fp32-reduce-scatter] [--cpu-offload]\n",
      "                     [--use-sharded-state] [--not-fsdp-flatten-parameters]\n",
      "                     [--arch ARCH] [--max-epoch MAX_EPOCH]\n",
      "                     [--max-update MAX_UPDATE]\n",
      "                     [--stop-time-hours STOP_TIME_HOURS]\n",
      "                     [--clip-norm CLIP_NORM] [--sentence-avg]\n",
      "                     [--update-freq UPDATE_FREQ] [--lr LR]\n",
      "                     [--stop-min-lr STOP_MIN_LR] [--use-bmuf]\n",
      "                     [--skip-remainder-batch] [--save-dir SAVE_DIR]\n",
      "                     [--restore-file RESTORE_FILE]\n",
      "                     [--continue-once CONTINUE_ONCE]\n",
      "                     [--finetune-from-model FINETUNE_FROM_MODEL]\n",
      "                     [--reset-dataloader] [--reset-lr-scheduler]\n",
      "                     [--reset-meters] [--reset-optimizer]\n",
      "                     [--optimizer-overrides OPTIMIZER_OVERRIDES]\n",
      "                     [--save-interval SAVE_INTERVAL]\n",
      "                     [--save-interval-updates SAVE_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]\n",
      "                     [--keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN]\n",
      "                     [--keep-last-epochs KEEP_LAST_EPOCHS]\n",
      "                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]\n",
      "                     [--no-save] [--no-epoch-checkpoints]\n",
      "                     [--no-last-checkpoints] [--no-save-optimizer-state]\n",
      "                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]\n",
      "                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]\n",
      "                     [--checkpoint-suffix CHECKPOINT_SUFFIX]\n",
      "                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]\n",
      "                     [--load-checkpoint-on-all-dp-ranks]\n",
      "                     [--write-checkpoints-asynchronously] [--store-ema]\n",
      "                     [--ema-decay EMA_DECAY]\n",
      "                     [--ema-start-update EMA_START_UPDATE]\n",
      "                     [--ema-seed-model EMA_SEED_MODEL]\n",
      "                     [--ema-update-freq EMA_UPDATE_FREQ] [--ema-fp32]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --no-progress-bar     disable progress bar\n",
      "  --log-interval LOG_INTERVAL\n",
      "                        log progress every N batches (when progress bar is\n",
      "                        disabled)\n",
      "  --log-format {json,none,simple,tqdm}\n",
      "                        log format to use\n",
      "  --log-file LOG_FILE   log file to copy metrics to.\n",
      "  --aim-repo AIM_REPO   path to Aim repository\n",
      "  --aim-run-hash AIM_RUN_HASH\n",
      "                        Aim run hash. If skipped, creates or continues run\n",
      "                        based on save_dir\n",
      "  --tensorboard-logdir TENSORBOARD_LOGDIR\n",
      "                        path to save logs for tensorboard, should match\n",
      "                        --logdir of running tensorboard (default: no\n",
      "                        tensorboard logging)\n",
      "  --wandb-project WANDB_PROJECT\n",
      "                        Weights and Biases project name to use for logging\n",
      "  --azureml-logging     Log scalars to AzureML context\n",
      "  --seed SEED           pseudo random number generator seed\n",
      "  --cpu                 use CPU instead of CUDA\n",
      "  --tpu                 use TPU instead of CUDA\n",
      "  --bf16                use bfloat16; implies --tpu\n",
      "  --memory-efficient-bf16\n",
      "                        use a memory-efficient version of BF16 training;\n",
      "                        implies --bf16\n",
      "  --fp16                use FP16\n",
      "  --memory-efficient-fp16\n",
      "                        use a memory-efficient version of FP16 training;\n",
      "                        implies --fp16\n",
      "  --fp16-no-flatten-grads\n",
      "                        don't flatten FP16 grads tensor\n",
      "  --fp16-init-scale FP16_INIT_SCALE\n",
      "                        default FP16 loss scale\n",
      "  --fp16-scale-window FP16_SCALE_WINDOW\n",
      "                        number of updates before increasing loss scale\n",
      "  --fp16-scale-tolerance FP16_SCALE_TOLERANCE\n",
      "                        pct of updates that can overflow before decreasing the\n",
      "                        loss scale\n",
      "  --on-cpu-convert-precision\n",
      "                        if set, the floating point conversion to fp16/bf16\n",
      "                        runs on CPU. This reduces bus transfer time and GPU\n",
      "                        memory usage.\n",
      "  --min-loss-scale MIN_LOSS_SCALE\n",
      "                        minimum FP16/AMP loss scale, after which training is\n",
      "                        stopped\n",
      "  --threshold-loss-scale THRESHOLD_LOSS_SCALE\n",
      "                        threshold FP16 loss scale from below\n",
      "  --amp                 use automatic mixed precision\n",
      "  --amp-batch-retries AMP_BATCH_RETRIES\n",
      "                        number of retries of same batch after reducing loss\n",
      "                        scale with AMP\n",
      "  --amp-init-scale AMP_INIT_SCALE\n",
      "                        default AMP loss scale\n",
      "  --amp-scale-window AMP_SCALE_WINDOW\n",
      "                        number of updates before increasing AMP loss scale\n",
      "  --user-dir USER_DIR   path to a python module containing custom extensions\n",
      "                        (tasks and/or architectures)\n",
      "  --empty-cache-freq EMPTY_CACHE_FREQ\n",
      "                        how often to clear the PyTorch CUDA cache (0 to\n",
      "                        disable)\n",
      "  --all-gather-list-size ALL_GATHER_LIST_SIZE\n",
      "                        number of bytes reserved for gathering stats from\n",
      "                        workers\n",
      "  --model-parallel-size MODEL_PARALLEL_SIZE\n",
      "                        total number of GPUs to parallelize model over\n",
      "  --quantization-config-path QUANTIZATION_CONFIG_PATH\n",
      "                        path to quantization config file\n",
      "  --profile             enable autograd profiler emit_nvtx\n",
      "  --reset-logging       when using Hydra, reset the logging at the beginning\n",
      "                        of training\n",
      "  --suppress-crashes    suppress crashes when training with the hydra_train\n",
      "                        entry point so that the main method can return a value\n",
      "                        (useful for sweeps)\n",
      "  --use-plasma-view     Store indices and sizes in shared memory\n",
      "  --plasma-path PLASMA_PATH\n",
      "                        path to run plasma_store, defaults to /tmp/plasma.\n",
      "                        Paths outside /tmp tend to fail.\n",
      "  --criterion {adaptive_loss,composite_loss,cross_entropy,ctc,fastspeech2,hubert,label_smoothed_cross_entropy,latency_augmented_label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,label_smoothed_cross_entropy_with_ctc,legacy_masked_lm_loss,masked_lm,model,nat_loss,sentence_prediction,sentence_prediction_adapters,sentence_ranking,tacotron2,speech_to_unit,speech_to_spectrogram,speech_unit_lm_criterion,wav2vec,vocab_parallel_cross_entropy}\n",
      "  --tokenizer {moses,nltk,space}\n",
      "  --bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}\n",
      "  --optimizer {adadelta,adafactor,adagrad,adam,adamax,composite,cpu_adam,lamb,nag,sgd}\n",
      "  --lr-scheduler {cosine,fixed,inverse_sqrt,manual,pass_through,polynomial_decay,reduce_lr_on_plateau,step,tri_stage,triangular}\n",
      "  --scoring {bert_score,sacrebleu,bleu,chrf,meteor,wer}\n",
      "  --task TASK           task\n",
      "\n",
      "dataset_data_loading:\n",
      "  --num-workers NUM_WORKERS\n",
      "                        how many subprocesses to use for data loading\n",
      "  --skip-invalid-size-inputs-valid-test\n",
      "                        ignore too long or too short lines in valid and test\n",
      "                        set\n",
      "  --max-tokens MAX_TOKENS\n",
      "                        maximum number of tokens in a batch\n",
      "  --batch-size BATCH_SIZE, --max-sentences BATCH_SIZE\n",
      "                        number of examples in a batch\n",
      "  --required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE\n",
      "                        batch size will be a multiplier of this value\n",
      "  --required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE\n",
      "                        maximum sequence length in batch will be a multiplier\n",
      "                        of this value\n",
      "  --dataset-impl {raw,lazy,cached,mmap,fasta,huffman}\n",
      "                        output dataset implementation\n",
      "  --data-buffer-size DATA_BUFFER_SIZE\n",
      "                        Number of batches to preload\n",
      "  --train-subset TRAIN_SUBSET\n",
      "                        data subset to use for training (e.g. train, valid,\n",
      "                        test)\n",
      "  --valid-subset VALID_SUBSET\n",
      "                        comma separated list of data subsets to use for\n",
      "                        validation (e.g. train, valid, test)\n",
      "  --combine-valid-subsets, --combine-val\n",
      "                        comma separated list of data subsets to use for\n",
      "                        validation (e.g. train, valid, test)\n",
      "  --ignore-unused-valid-subsets\n",
      "                        do not raise error if valid subsets are ignored\n",
      "  --validate-interval VALIDATE_INTERVAL\n",
      "                        validate every N epochs\n",
      "  --validate-interval-updates VALIDATE_INTERVAL_UPDATES\n",
      "                        validate every N updates\n",
      "  --validate-after-updates VALIDATE_AFTER_UPDATES\n",
      "                        dont validate until reaching this many updates\n",
      "  --fixed-validation-seed FIXED_VALIDATION_SEED\n",
      "                        specified random seed for validation\n",
      "  --disable-validation  disable validation\n",
      "  --max-tokens-valid MAX_TOKENS_VALID\n",
      "                        maximum number of tokens in a validation batch\n",
      "                        (defaults to --max-tokens)\n",
      "  --batch-size-valid BATCH_SIZE_VALID, --max-sentences-valid BATCH_SIZE_VALID\n",
      "                        batch size of the validation batch (defaults to\n",
      "                        --batch-size)\n",
      "  --max-valid-steps MAX_VALID_STEPS, --nval MAX_VALID_STEPS\n",
      "                        How many batches to evaluate\n",
      "  --curriculum CURRICULUM\n",
      "                        don't shuffle batches for first N epochs\n",
      "  --gen-subset GEN_SUBSET\n",
      "                        data subset to generate (train, valid, test)\n",
      "  --num-shards NUM_SHARDS\n",
      "                        shard generation over N shards\n",
      "  --shard-id SHARD_ID   id of the shard to generate (id < num_shards)\n",
      "  --grouped-shuffling   shuffle batches in groups of num_shards to enable\n",
      "                        similar sequence lengths on each GPU worker when\n",
      "                        batches are sorted by length\n",
      "  --update-epoch-batch-itr UPDATE_EPOCH_BATCH_ITR\n",
      "                        if true then prevents the reuse the epoch batch\n",
      "                        iterator by setting can_reuse_epoch_itr to false,\n",
      "                        defaults to --grouped-shuffling )\n",
      "  --update-ordered-indices-seed\n",
      "                        if true then increment seed with epoch for getting\n",
      "                        batch iterators, defautls to False.\n",
      "\n",
      "distributed_training:\n",
      "  --distributed-world-size DISTRIBUTED_WORLD_SIZE\n",
      "                        total number of GPUs across all nodes (default: all\n",
      "                        visible GPUs)\n",
      "  --distributed-num-procs DISTRIBUTED_NUM_PROCS\n",
      "                        total number of processes to fork (default: all\n",
      "                        visible GPUs)\n",
      "  --distributed-rank DISTRIBUTED_RANK\n",
      "                        rank of the current worker\n",
      "  --distributed-backend DISTRIBUTED_BACKEND\n",
      "                        distributed backend\n",
      "  --distributed-init-method DISTRIBUTED_INIT_METHOD\n",
      "                        typically tcp://hostname:port that will be used to\n",
      "                        establish initial connetion\n",
      "  --distributed-port DISTRIBUTED_PORT\n",
      "                        port number (not required if using --distributed-init-\n",
      "                        method)\n",
      "  --device-id DEVICE_ID, --local_rank DEVICE_ID\n",
      "                        which GPU to use (by default looks for $LOCAL_RANK,\n",
      "                        usually configured automatically)\n",
      "  --distributed-no-spawn\n",
      "                        do not spawn multiple processes even if multiple GPUs\n",
      "                        are visible\n",
      "  --ddp-backend {c10d,fully_sharded,legacy_ddp,no_c10d,pytorch_ddp,slowmo}\n",
      "                        DistributedDataParallel backend\n",
      "  --ddp-comm-hook {none,fp16}\n",
      "                        communication hook\n",
      "  --bucket-cap-mb BUCKET_CAP_MB\n",
      "                        bucket size for reduction\n",
      "  --fix-batches-to-gpus\n",
      "                        don't shuffle batches between GPUs; this reduces\n",
      "                        overall randomness and may affect precision but avoids\n",
      "                        the cost of re-reading the data\n",
      "  --find-unused-parameters\n",
      "                        disable unused parameter detection (not applicable to\n",
      "                        --ddp-backend=legacy_ddp)\n",
      "  --gradient-as-bucket-view\n",
      "                        when set to True, gradients will be views pointing to\n",
      "                        different offsets of allreduce communication buckets.\n",
      "                        This can reduce peak memory usage, where the saved\n",
      "                        memory size will be equal to the total gradients size.\n",
      "                        --gradient-as-bucket-view=gradient_as_bucket_view)\n",
      "  --fast-stat-sync      [deprecated] this is now defined per Criterion\n",
      "  --heartbeat-timeout HEARTBEAT_TIMEOUT\n",
      "                        kill the job if no progress is made in N seconds; set\n",
      "                        to -1 to disable\n",
      "  --broadcast-buffers   Copy non-trainable parameters between GPUs, such as\n",
      "                        batchnorm population statistics\n",
      "  --slowmo-momentum SLOWMO_MOMENTUM\n",
      "                        SlowMo momentum term; by default use 0.0 for 16 GPUs,\n",
      "                        0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs\n",
      "  --slowmo-base-algorithm SLOWMO_BASE_ALGORITHM\n",
      "                        Base algorithm. Either 'localsgd' or 'sgp'. Please\n",
      "                        refer to the documentation of 'slowmo_base_algorithm'\n",
      "                        parameter in https://fairscale.readthedocs.io/en/lates\n",
      "                        t/api/experimental/nn/slowmo_ddp.html for more details\n",
      "  --localsgd-frequency LOCALSGD_FREQUENCY\n",
      "                        Local SGD allreduce frequency\n",
      "  --nprocs-per-node NPROCS_PER_NODE\n",
      "                        number of GPUs in each node. An allreduce operation\n",
      "                        across GPUs in a node is very fast. Hence, we do\n",
      "                        allreduce across GPUs in a node, and gossip across\n",
      "                        different nodes\n",
      "  --pipeline-model-parallel\n",
      "                        if set, use pipeline model parallelism across GPUs\n",
      "  --pipeline-balance PIPELINE_BALANCE\n",
      "                        partition the model into N_K pieces, where each piece\n",
      "                        contains N_i layers. The sum(args.pipeline_balance)\n",
      "                        should equal the total number of layers in the model\n",
      "  --pipeline-devices PIPELINE_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-balance\n",
      "                        argument\n",
      "  --pipeline-chunks PIPELINE_CHUNKS\n",
      "                        microbatch count for pipeline model parallelism\n",
      "  --pipeline-encoder-balance PIPELINE_ENCODER_BALANCE\n",
      "                        partition the pipeline parallel encoder into N_K\n",
      "                        pieces, where each piece contains N_i layers. The\n",
      "                        sum(args.pipeline_encoder_balance) should equal the\n",
      "                        total number of encoder layers in the model\n",
      "  --pipeline-encoder-devices PIPELINE_ENCODER_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-\n",
      "                        encoder-balance argument\n",
      "  --pipeline-decoder-balance PIPELINE_DECODER_BALANCE\n",
      "                        partition the pipeline parallel decoder into N_K\n",
      "                        pieces, where each piece contains N_i layers. The\n",
      "                        sum(args.pipeline_decoder_balance) should equal the\n",
      "                        total number of decoder layers in the model\n",
      "  --pipeline-decoder-devices PIPELINE_DECODER_DEVICES\n",
      "                        a list of device indices indicating which device to\n",
      "                        place each of the N_K partitions. The length of this\n",
      "                        list should equal the length of the --pipeline-\n",
      "                        decoder-balance argument\n",
      "  --pipeline-checkpoint {always,never,except_last}\n",
      "                        checkpointing mode for pipeline model parallelism\n",
      "  --zero-sharding {none,os}\n",
      "                        ZeRO sharding\n",
      "  --no-reshard-after-forward\n",
      "                        don't reshard parameters after forward pass\n",
      "  --fp32-reduce-scatter\n",
      "                        reduce-scatter grads in FP32\n",
      "  --cpu-offload         offload FP32 params to CPU\n",
      "  --use-sharded-state   use sharded checkpoint files\n",
      "  --not-fsdp-flatten-parameters\n",
      "                        not flatten parameter param for fsdp\n",
      "\n",
      "Model configuration:\n",
      "  --arch ARCH, -a ARCH  model architecture\n",
      "\n",
      "optimization:\n",
      "  --max-epoch MAX_EPOCH\n",
      "                        force stop training at specified epoch\n",
      "  --max-update MAX_UPDATE\n",
      "                        force stop training at specified update\n",
      "  --stop-time-hours STOP_TIME_HOURS\n",
      "                        force stop training after specified cumulative time\n",
      "                        (if >0)\n",
      "  --clip-norm CLIP_NORM\n",
      "                        clip threshold of gradients\n",
      "  --sentence-avg        normalize gradients by the number of sentences in a\n",
      "                        batch (default is to normalize by number of tokens)\n",
      "  --update-freq UPDATE_FREQ\n",
      "                        update parameters every N_i batches, when in epoch i\n",
      "  --lr LR               learning rate for the first N epochs; all epochs >N\n",
      "                        using LR_N (note: this may be interpreted differently\n",
      "                        depending on --lr-scheduler)\n",
      "  --stop-min-lr STOP_MIN_LR\n",
      "                        stop training when the learning rate reaches this\n",
      "                        minimum\n",
      "  --use-bmuf            specify global optimizer for syncing models on\n",
      "                        different GPUs/shards\n",
      "  --skip-remainder-batch\n",
      "                        if set, include the last (partial) batch of each epoch\n",
      "                        in training (default is to skip it).\n",
      "\n",
      "checkpoint:\n",
      "  --save-dir SAVE_DIR   path to save checkpoints\n",
      "  --restore-file RESTORE_FILE\n",
      "                        filename from which to load checkpoint (default:\n",
      "                        <save-dir>/checkpoint_last.pt\n",
      "  --continue-once CONTINUE_ONCE\n",
      "                        continues from this checkpoint, unless a checkpoint\n",
      "                        indicated in 'restore_file' option is present\n",
      "  --finetune-from-model FINETUNE_FROM_MODEL\n",
      "                        finetune from a pretrained model; note that meters and\n",
      "                        lr scheduler will be reset\n",
      "  --reset-dataloader    if set, does not reload dataloader state from the\n",
      "                        checkpoint\n",
      "  --reset-lr-scheduler  if set, does not load lr scheduler state from the\n",
      "                        checkpoint\n",
      "  --reset-meters        if set, does not load meters from the checkpoint\n",
      "  --reset-optimizer     if set, does not load optimizer state from the\n",
      "                        checkpoint\n",
      "  --optimizer-overrides OPTIMIZER_OVERRIDES\n",
      "                        a dictionary used to override optimizer args when\n",
      "                        loading a checkpoint\n",
      "  --save-interval SAVE_INTERVAL\n",
      "                        save a checkpoint every N epochs\n",
      "  --save-interval-updates SAVE_INTERVAL_UPDATES\n",
      "                        save a checkpoint (and validate) every N updates\n",
      "  --keep-interval-updates KEEP_INTERVAL_UPDATES\n",
      "                        keep the last N checkpoints saved with --save-\n",
      "                        interval-updates\n",
      "  --keep-interval-updates-pattern KEEP_INTERVAL_UPDATES_PATTERN\n",
      "                        when used with --keep-interval-updates, skips deleting\n",
      "                        any checkpoints with update X where X %\n",
      "                        keep_interval_updates_pattern == 0\n",
      "  --keep-last-epochs KEEP_LAST_EPOCHS\n",
      "                        keep last N epoch checkpoints\n",
      "  --keep-best-checkpoints KEEP_BEST_CHECKPOINTS\n",
      "                        keep best N checkpoints based on scores\n",
      "  --no-save             don't save models or checkpoints\n",
      "  --no-epoch-checkpoints\n",
      "                        only store last and best checkpoints\n",
      "  --no-last-checkpoints\n",
      "                        don't store last checkpoints\n",
      "  --no-save-optimizer-state\n",
      "                        don't save optimizer-state as part of checkpoint\n",
      "  --best-checkpoint-metric BEST_CHECKPOINT_METRIC\n",
      "                        metric to use for saving \"best\" checkpoints\n",
      "  --maximize-best-checkpoint-metric\n",
      "                        select the largest metric value for saving \"best\"\n",
      "                        checkpoints\n",
      "  --patience PATIENCE   early stop training if valid performance doesn't\n",
      "                        improve for N consecutive validation runs; note that\n",
      "                        this is influenced by --validate-interval\n",
      "  --checkpoint-suffix CHECKPOINT_SUFFIX\n",
      "                        suffix to add to the checkpoint file name\n",
      "  --checkpoint-shard-count CHECKPOINT_SHARD_COUNT\n",
      "                        Number of shards containing the checkpoint - if the\n",
      "                        checkpoint is over 300GB, it is preferable to split it\n",
      "                        into shards to prevent OOM on CPU while loading the\n",
      "                        checkpoint\n",
      "  --load-checkpoint-on-all-dp-ranks\n",
      "                        load checkpoints on all data parallel devices\n",
      "                        (default: only load on rank 0 and broadcast to other\n",
      "                        devices)\n",
      "  --write-checkpoints-asynchronously, --save-async\n",
      "                        Write checkpoints asynchronously in a separate thread.\n",
      "                        NOTE: This feature is currently being tested.\n",
      "\n",
      "EMA configuration:\n",
      "  --store-ema\n",
      "  --ema-decay EMA_DECAY\n",
      "                        decay for exponential moving average model\n",
      "  --ema-start-update EMA_START_UPDATE\n",
      "                        start EMA update after this many model updates\n",
      "  --ema-seed-model EMA_SEED_MODEL\n",
      "                        Seed to load EMA model from. Used to load EMA model\n",
      "                        separately from the actual model.\n",
      "  --ema-update-freq EMA_UPDATE_FREQ\n",
      "                        Do EMA update every this many model updates\n",
      "  --ema-fp32            If true, store EMA model in fp32 even if model is in\n",
      "                        fp16\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T15:46:31.631889Z",
     "start_time": "2024-07-11T15:46:31.332644Z"
    }
   },
   "cell_type": "code",
   "source": "%run ping 8.8.8.8",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'ping'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\magics\\execution.py:716\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[1;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[0;32m    715\u001B[0m     fpath \u001B[38;5;241m=\u001B[39m arg_lst[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m--> 716\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[43mfile_finder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    717\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\utils\\path.py:91\u001B[0m, in \u001B[0;36mget_py_filename\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m     90\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m py_name\n\u001B[1;32m---> 91\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile `\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m` not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m name)\n",
      "\u001B[1;31mOSError\u001B[0m: File `'ping'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mping 8.8.8.8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2480\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2478\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2479\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2480\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2482\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2483\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2484\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\magics\\execution.py:727\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[1;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[0;32m    725\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.*\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m\"\u001B[39m,fpath):\n\u001B[0;32m    726\u001B[0m         warn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFor Windows, use double quotes to wrap a filename: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124mun \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmypath\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmyfile.py\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 727\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    728\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    729\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fpath \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmeta_path:\n",
      "\u001B[1;31mException\u001B[0m: File `'ping'` not found."
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "\n",
    "\n",
    "@register_line_magic\n",
    "def runrealcmd(command):\n",
    "    process = Popen(command, stdout=PIPE, shell=True, stderr=STDOUT, bufsize=1, close_fds=True)\n",
    "    for line in iter(process.stdout.readline, b''):\n",
    "        print(line.rstrip().decode('utf-8'))\n",
    "    process.stdout.close()\n",
    "    process.wait()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
